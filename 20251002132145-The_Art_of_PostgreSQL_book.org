:PROPERTIES:
:ID: 0ec0df6d-941f-40ff-9dee-bc56c521e53b
:BOOK_URL:
[[https://storage.sbg.cloud.ovh.net/v1/AUTH_e5524010dbdf45ccb5cdac68b254c4f7/TAOP/TAOP-Volume-1.pdf]]
:ROAM_ALIASES: "taop"
:END:
#+TITLE: The Art of PostgreSQL book

* 1. Structured Query Language (SQL)
  RDBMS (Relational DataBase Management System) and SQL are forcing developers
  to think in terms of data structure,
  and to declare both the data structure and the data set we want to obain
  via our queries.

  #+begin_quote
  Bad programmers worry about the code. \\  
  Good programmers worry about data structures and their relationships.

  --- Linus Torvalds
  #+end_quote
** Some of the Code is Written in SQL
   The current SQL standard is SQL:2016.

   If your application is already using the SQL programming language
   and SQL engine, then as a developer it's important to fully understand how 
   much can be achieved in SQL, and what service is implemented by this runtime
   dependency in your software architecture.

   SQL is a very powerful programming language, and it is a declarative one.
   It's a wonderful tool to master, and once used properly it allows one to
   reduce
   both code size and the development time for new features.

** A First Use Case
   Fetch NYSE /Excel/ file and load it into a PostgreSQL table.

   How file looks:
   #+begin_example
   2010  1/4/2010    1,425,504,460  4,628,115   $38.495.460,645
   2010  1/5/2010    1,754,011,760  5,394,016   $43.932.043,406
   2010  1/6/2010    1,655,507,953  5,494,460   $43.816.749,660
   #+end_example

   #+begin_src sql 
   begin;

   drop table if exists factbook;

   create table factbook
   (
      year int,
      date date,
      shares text,
      trades text,
      dollars text
   );

   \copy factbook from 'factbook.csv' with delimiter E'\t' null ''

   alter table factbook
   alter shares
   type bigint
   using replace(shares, ',', '')::bigint,

   alter trades
   type bigint
   using replace(trades, ',', '')::bigint,

   alter dollars
   type bigint
   using substring(replace(dollars, ',', '') from 2)::numeric;

   commit;
   #+end_src


** Application Code and SQL
   Query that list all entries we have in the month of February 2017:
   #+begin_src sql
   \set start '2017-02-01'

   select date, to_char(shares, '99G999G999G999') as shares, to_char(trades,
   '99G999G999') as trades, to_char(dollars, 'L99G999G999G999') as dollars
   from factbook
   where date >= date :'start'
   and date < date :'start' + interval '1 month'
   order by date;
   #+end_src

   Result of the query:
   | date       | shares        | trades    | dollars          |
   |------------+---------------+-----------+------------------|
   | 2017-02-01 | 1,161,001,502 | 5,217,859 | $ 44,660,060,305 |
   | (n rows)   |               |           |                  |

   Typical implementation of that expectation in Python
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg-binary",
   # ]
   # ///
   from datetime import date
   import sys
   import psycopg2
   import psycopg2.extras
   from calendar import Calendar

   CONNSTRING = "dbname=yesql application_name=factbook"


   def fetch_month_data(year: int, month: int) -> dict[date, tuple[str, str,
       str]]:
       date = "%d-%02d-01" % (year, month)
       sql = """
   select date, shared, trades, dollars
   from factbook
   where date >= date %s
   and date < date %s + interval '1 month'
   order by date;
   """
       pgconn = psycopg2.connect(CONNSTRING)
       curs = pgconn.cursor()
       curs.execute(sql, (date, date))

       return {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in curs.fetchall()
       }


   def list_book_for_month(year: int, month: int):
       data = fetch_month_data(year, month)
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades",
                                            "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" *
           12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src

   Output when running the program
   #+begin_src sh
   $ uv run main.py 2017 2
   | day        | shares     | trades  | dollars     |
      |------------+------------+---------+-------------|
      | 2017-02-01 | 1161001502 | 5217859 | 44660060305 |
      | etc
   #+end_src
** A Word about SQL Injection
   [[https://imgs.xkcd.com/comics/exploits_of_a_mom.png]]

   It is advisable that read the documentation of current driver and understand
   how to send
   SQL query parameters separately from the main SQL query text;
   this is a reliable way to never have to worry about SQL injection problems
   ever again.

   In particular, never build a query string by concatenating query arguments directly
   into query strings, i.e. in the application client code.
   Never use library, ORM or another tooling that would do that.

   We were using the psycopg Python driver which is based on *libpq*. \\  
   A lot of PostgreSQL application drivers are based on the libpq C driver,
   which
   implements the PostgreSQL protocols and is mantained alongside the main
   server's code.


** PostgreSQL protocol: server-side prepared statements
   Server-side Prepared Statements can be used in SQL thanks to the *PREPARE*
   and *EXECUTE* commands syntax:
   #+begin_src sql
   prepare foo as
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;

   -- And then execute the prepared statement with a parameter
   execute foo('2010-02-01') 
   #+end_src

   Remember: SQL injection happens when the SQL parser is fooled into beliving
   that
   a parameter string is in fact a SQL query, and then the SQL engine goes on
   and
   execute that SQL statement.

   *asyncpg* PostgreSQL driver that implements the PostgreSQL protocol itself,
   and uses
   server-side prepared statements.

   This example is now safe from SQL injection by design, because the
   server-side
   prepared statement protocol sends the query string and its arguments in
   separate protocol
   messages:
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "asyncpg",
   # ]
   # ///
   import sys
   import asyncio
   import asyncpg
   import datetime
   from calendar import Calendar

   CONNSTRING =
   "postgresql://appdev@localhost/appdev?application_name=factbook"


   async def fetch_month_data(year: int, month: int):
       date = datetime.date(year, month, 1)
       sql = """
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;
   """
       pgconn = await asyncpg.connect(CONNSTRING)
       stmt = await pgconn.prepare(sql)
       res = {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in stmt.fetch(date)
       }
       await pgconn.close()
       return res


   def list_book_for_month(year: int, month: int):
       data = asyncio.run(fetch_month_data(year, month))
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades",
                                            "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" *
           12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src


** Back to Discovering SQL
   Now of course it's possible to implement the same expectations with a single
   SQL
   query, without any application code being spent on solving the problem:
   #+begin_src sql
   select cast(calendar.entry as date) as date,
      coalesce(shares, 0) as shares,
      coalesce(trades, 0) as trades,
      to_char(
      coalesce(dollars,0),
      'L99G999G999G999'
      ) as dollars
   from
   generate_series(date :'start',
   date :'start' + interval '1 month'
   - interval '1 day',
   interval '1 day'
   )
   as calendar(entry)
   left join factbook
   on factbook.date = calendar.entry
   order by date;
   #+end_src

   Here's the result of running this query:
   | date       | shares     | trades  | dollars          |
   |------------+------------+---------+------------------|
   | 2017-02-01 | 1161001502 | 5217859 | $ 44,660,060,305 |
   | 2017-02-02 | 1128144760 | 4586343 | $ 43,276,102,903 |
   | etc        |            |         |                  |

   Note that we replaced 60 lines of Python code with a simple enough SQL query
   Here, the Python is doing and /Hash Join Nested Loop/ where PostgreSQL picks
   a 
   /Merge Left Join/ over two ordered relations.

* 2. Software Architecture
  When designing your software architecture, you need to think about PostgreSQL
  not as /storage/ layer, but rather as a /concurrent data access service/.
  This service is capable of handling data processing.

** Why PostgreSQL? 
   That choice is down to several factors, all consequences of PostgreSQL
   truly being /the world's most advanced open source database/:
   - PostgreSQL is open source, available under a BSD like licence
     named the *PostgreSQL licence*
   - The PostgreSQL project is done completely in the open. \\  
     The project goes as far as self-hosting all requirements in order
     to avoid being influenced by a particular company.
   - PostgreSQL releases a new major version about once a year, following a
     /when it's ready/ releaase cycle
   - The PostgreSQL design allows enhancing SQL in very advanced ways


* 3. Getting Ready to read this Book
  #+begin_src sh
  $ docker run -d -p 127.0.0.1:5432:5432 -e POSTGRES_PASSWORD="1234" --name
  the_art_of_postgresql_book postgres:16.9-alpine
  $ docker exec -it the_art_of_postgresql_book psql -U postgres
  postgres=# show server_version;
  server_version 
  ----------------
  16.9
  (1 row)
  #+end_src

  Need to import /Chinook/ database to PostgreSQL:
  #+begin_src sh
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -c "create
  database appdev"
  $ curl -fsSL
  "https://github.com/edpyt/TAOP-sql/raw/3a7c45887a4223a730a7659cb2325990d0696cfd/TheArtOfPostgreSQL-database-sql/chinook.sql"
  | docker exec -i the_art_of_postgresql_book psql -U postgres -d appdev 
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -d appdev -c
  "ALTER ROLE postgres SET search_path TO chinook;"
  #+end_src
* 4. Business Logic

** Every SQL query embeds some business logic
   Each and every and all SQL query contains some levels of business logic.

   Example:
   #+begin_src sql
   select name
   from track
   where albumid = 193
   order by trackid;
   #+end_src
   What business logic is embedded in that SQL statement?
   - The /select/ clause only mentions the /name/ column, and that's relevant
     to your application.
     In the situation in which your application runs this query, the business
     logic
     is only interested into the tracks names.
   - The /from/ clause only mentions the /track/ trable, somehow we decided that's
     all we need in this example, and that again is strongly tied to the logic being
     implemented
   - The /where/ clause restricts the data output to the /albumid/ 193 which
     again
     is a direct translation of our business logic, with the added information
     that the album we want now is the 193rd one and we're left to wonder how 
     we know about it
   - Finally, the /order by/ clause implements the idead that we want to
     display
     the track names in the order they appear on the disk.
     Not only that, it also incorporates the specific knowledge that the
     /trackid/
     column ordering is the same as the original disk ordering of the tracks.
** Business Logic Applies to Use Cases
   Display the list of albums from a given artist, each with its total
   duration.
   #+begin_src sql
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.name = 'Red Hot Chili Peppers'
   group by album
   order by album;
   #+end_src
   The output is:
   | album                 | duration     |
   |-----------------------+--------------|
   | Blood Sugar Sex Magik | 01:13:57.073 |
   | By The Way            | 01:08:49.951 |
   | Californication       | 00:56:25.461 |

   What we see here is a direct translation from the business case (or user
   story)
   into a SQL query. The SQL implementation uses joins and computations that
   are specific
   to both the data model and the use case we are solving.

   Another implementation could be done with several queries and the
   computation
   in the application's main code:
   1. Fetch the list of albums for the selected artist
   2. For each album, fetch the duration of every track in the album
   3. In the application, sum up the durations per album

   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg[binary]",
   # ]
   # ///
   from dataclasses import dataclass
   from typing import ClassVar, Self
   import psycopg
   import sys
   from datetime import timedelta

   from psycopg.rows import dict_row

   DEBUGSQL = False
   PGCONNSTRING = "host=localhost user=postgres password=1234 dbname=appdev"


   @dataclass
   class Model:
       tablename: ClassVar[str | None] = None
       columns: ClassVar[list[str]] = []

       @classmethod
       def buildsql(cls, pgconn: psycopg.Connection, **kwargs) -> str | None:
           if cls.tablename and kwargs:
               cols = ", ".join('"%s"' % c for c in cls.columns)
               qtab = '"%s"' % cls.tablename
               sql = "select %s from %s where" % (cols, qtab)
               for key in kwargs.keys():
                   sql += "\"%s\" = '%s'" % (key, kwargs[key])
                   if DEBUGSQL:
                       print(sql)
                   return sql

       @classmethod
       def fetchone(cls, pgconn: psycopg.Connection, **kwargs) -> Self | None:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if (result := curs.fetchone()) is not None:
                   return cls(*result.values())

       @classmethod
       def fetchall(cls, pgconn: psycopg.Connection, **kwargs) -> list[Self]:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if resultset := curs.fetchall():
                   return [cls(*result.values()) for result in resultset]
           return []


   @dataclass
   class Artist(Model):
       tablename = "artist"
       columns = ["artistid", "name"]

       id: int
       title: str


   @dataclass
   class Album(Model):
       tablename = "album"
       columns = ["albumid", "title"]

       id: int
       title: str
       duration: float | None = None


   @dataclass
   class Track(Model):
       tablename = "track"
       columns = ["trackid", "name", "milliseconds", "bytes", "unitprice"]

       id: int
       name: str
       duration: int
       bytes: float
       unitprice: int


   if __name__ == "__main__":
       if len(sys.argv) > 1:
           pgconn = psycopg.connect(PGCONNSTRING)
           artist = Artist.fetchone(pgconn, name=sys.argv[1])
           for album in Album.fetchall(pgconn, artistid=artist.id):
               ms = 0
               for track in Track.fetchall(pgconn, albumid=album.id):
                   ms += track.duration
                   duration = timedelta(milliseconds=ms)
               print("%25s: %s" % (album.title, duration))
       else:
           print("albums.py <artist name>")
   #+end_src

   Now the result of this code is as following:
   #+begin_src sh
   $ uv run main.py "Red Hot Chili Peppers"
   Blood Sugar Sex Magik: 1:13:57.073000
   By The Way: 1:08:49.951000
   Californication: 0:56:25.461000
   #+end_src
** Correctness
   When using multiple statements, it is necessary to setup the /isolation
   level/ correctly.
   Also, the connection and transaction semantice of your code should be
   tightly controlled.

   The SQL standard default four isolation level and PostgreSQL implements
   three of them,
   leaving out /dirty reads/.

   Think of the isolation levels like this:
   - Read uncommited \\  
     PostgreSQL accepts this setting and actually implements /read commited/
     here,
     which is compliant with the SQL standard
   - Read committed \\  
     This is the default and it allows your transaction to see other transactions
     changes as soon as they are committed; it means that if you run the
     following
     query twice in your transaction but someone else added or removed objects
     from the stock, you will have different count at different points in your
     transaction
     #+begin_src sql
     select count(*) from stock;
     #+end_src
   - Repeatable read \\  
     In this isolation level, your transaction keeps the same /snapshot/ of the
     whole database
     for its entire duration, from *BEGIN* to *COMMIT*.
     It is very useful to have that for online backups - a straightforward use
     case for this
     feature.
   - Serializable \\  
     This level guarantees that a one-transaction-at-a-time ordering of what
     happens on the server
     exists with the exact same result as what you're obtaining with concurrent
     activity.

   By default working in /read commited/ isolation level.

   Each running transaction in a PostgreSQL system can hae a different
   isolation level.

** Efficiency
   The correct soultion is eight lines of very basic SQL.

   In the application's code solution, here's what happens under the hood:
   - First, we fetch the artist from the database, so that's one network round
     trip
     and one SQL query that returns the artist id and its name

     note that we don't need the name of the artist in our use-save, so that's
     a useless
     amount of bytes sent on the network, and also in memory in the application
   - Then we do another network round-trip to fetch a list of albums for the
     artistid
     we just retrieved in the previous query, and store the result in the
     application's
     memory
   - Now for each album we send another SQL query via the network to the
     database server
     and fetch the list of tracks and their properties, including the duration
     in milliseconds.
   - In the same loop where we fetch the tracks durations in milliseconds, we sum them up
     in the application's memory - we can approximate the CPU usage on the
     application side
     to be the same as the one in the PostgreSQL server.
   - Finnaly, the application can output the fetched data
** Stored Procedures - a Data Access API
   When using PostgreSQL it is also possible to create server-side functions.
   #+begin_src sql
   create or replace function get_all_albums
   (
   in artistid bigint,
   out album text,
   out duration interval
   )
   returns setof record
   language sql
   as $$
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.artistid = get_all_albums.artistid
   group by album
   order by album;
   $$;
   #+end_src

   Then we can use this procedure with /lateral/ join technique:
   #+begin_src sql
   select album, duration
   from artist
   lateral get_all_albums(artistid)
   where artist.name = 'Red Hot Chili Peppers';
   #+end_src

   And example we want to list the album with durations of the artists
   who have exactly four albums registered in database:
   #+begin_src sql
   with four_albums as
      (
      select artistid
      from album
      group by artistid
      having count(*) = 4
   )
   select artist.name, album, duration
   from four_albums
   join artist using(artistid),
   lateral get_all_albums(artistid)
   order by artistid, duration desc;
   #+end_src
** Procedural Code and Stored Procedures
   If you want to use stored procedures, please always write them in SQL,
   and only switch to /PLpgSQL/ when necessary.
   If you want to be efficient, the default should be SQL.

* 5. A Small Application

** Music Catalog
   Using the [[https://github.com/nackjicholson/aiosql][aiosql]] Python library
   it is very
   easy to embed SQL code in Python and keep the SQL clean and tidy in .sql
   files.

   Needed .sql files can be found here:
   [[./assets/python/taop_book/chapter5/music_catalog/queries/][queries]]

* 6. The SQL REPL - An Interactive Setup
  New uses of PostgreSQL often want to find an andvanced visual query editing
  tool and are confused when /psql/ is the answer.
  Most PostgreSQL advanced users and experts don't even think about it and use
  /psql/.

  /psql/ implements a REPL (read-eval-print loop)

** The psqlrc Setup
   Full setup of /psql/ that being used:
   #+begin_src psqlrc
   -- Recommended psql config from Chapter 6: The SQL REPL — An Interactive
   Setup

   -- These set commands are noisy; let's shush 'em.
   -- \set QUIET ON

   \set PROMPT1 '%~%x%# '
   \x auto
   \set ON_ERROR_STOP on
   \set ON_ERROR_ROLLBACK interactive

   \pset null '¤'
   \pset linestyle 'unicode'
   \pset unicode_border_linestyle single
   \pset unicode_column_linestyle single
   \pset unicode_header_linestyle double
   \set intervalstyle to 'postgres_verbose';

   \setenv LESS '-iMFXSx4R'
   -- Take your pick: emacs, nano, vim, or install another
   \setenv EDITOR 'vim'

   \set QUIET OFF
   #+end_src
*** My setup with Docker
    #+begin_src bash
    $ docker exec -it the_art_of_postgresql_book apk add vim
    $ docker exec -it the_art_of_postgresql_book bash -c "wget -P ~/ \
       https://raw.githubusercontent.com/mikebranski/the-art-of-postgresql-docker/refs/heads/master/.psqlrc"
    #+end_src

** Transactions and psql Behavior 
   We set several /psql/ variables that change its behavior:
   - \set ON_ERROR_STOP on \\  
     It allows /psql/ to know that it is not to continue trying to execute all
     your
     commands when a previous one is throwing an error.
     It's primarily practical for scripts and be also set using the command
     line.
   - \set ON_ERROR_ROLLBACK interactive \\  
     This settings changes how /psql/ behaves with respect to transactions.
     It is a very good interactive setup, and must be avoided in batch scripts.

     From the documentation, when set to:
     - *on* \\  
       If a statement in a transaction block generates an error, the error is
       ignored and the transaction continues.
     - *interactive* \\  
       Such errors are only ignored in interactive sessions, and not when
       reading 
       script files.
     - *unset* or set to *off* \\  
       a statement in a transaction block that generates an error aborts the
       entire
       transaction.

   The error rollback mode works by issuing an implicit *SAVEPOINT*, just
   before each
   command that is in a transaction block, and then rolling back to the
   savepoint
   if the command fails.

   With the /\set PROMPT1 '%~%*%# '/, /psql/ displays a little star in the
   prompt
   when there's a transaction in flight.

   Example output with /ON_ERROR_ROLLBACK/ set to:
   - *off*
     #+begin_src
     appdev# \set ON_ERROR_ROLLBACK off
     appdev# begin;
     BEGIN
     appdev*# select 1/0;
     ERROR:  division by zero
     appdev!# select 1+1;
     ERROR:  current transaction is aborted, commands ignored until end of
     transaction block
     appdev!# rollback;
     ROLLBACK  
     #+end_src
   - *interactive*
     #+begin_src
     appdev# \set ON_ERROR_ROLLBACK interactive
     appdev# begin;
     BEGIN
     appdev*# select 1/0;
     ERROR:  division by zero
     appdev*# select 1+1;
     ?column? 
     ══════════
           2
     (1 row)

     appdev*# commit;
     COMMIT
     #+end_src

   For discover database schema use /\l+/:
   #+begin_src
   appdev# \set ECHO_HIDDEN true
   appdev# \l+
   ********* QUERY **********
   SELECT
   d.datname as "Name",
   pg_catalog.pg_get_userbyid(d.datdba) as "Owner",
   pg_catalog.pg_encoding_to_char(d.encoding) as "Encoding",
   CASE d.datlocprovider WHEN 'c' THEN 'libc' WHEN 'i' THEN 'icu' END AS
   "Locale Provider",
   d.datcollate as "Collate",
   d.datctype as "Ctype",
   d.daticulocale as "ICU Locale",
   d.daticurules as "ICU Rules",
   pg_catalog.array_to_string(d.datacl, E'\n') AS "Access privileges",
   CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT')
         THEN pg_catalog.pg_size_pretty(pg_catalog.pg_database_size(d.datname))
         ELSE 'No Access'
   END as "Size",
   t.spcname as "Tablespace",
   pg_catalog.shobj_description(d.oid, 'pg_database') as "Description"
   FROM pg_catalog.pg_database d
   JOIN pg_catalog.pg_tablespace t on d.dattablespace = t.oid
   ORDER BY 1;
   **************************

   List of databases
   ─[ RECORD 1 ]─────┬───────────────────────────────────────────
   Name              │ appdev
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   Collate           │ en_US.utf8
   Ctype             │ en_US.utf8
   ICU Locale        │ 
   ICU Rules         │ 
   Access privileges │ 
   Size              │ 24 MB
   Tablespace        │ pg_default
   Description       │ 
   ═[ RECORD 2 ]═════╪═══════════════════════════════════════════
   Name              │ postgres
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   Collate           │ en_US.utf8
   Ctype             │ en_US.utf8
   ICU Locale        │ 
   ICU Rules         │ 
   Access privileges │ 
   Size              │ 7361 kB
   Tablespace        │ pg_default
   Description       │ default administrative connection database
   ═[ RECORD 3 ]═════╪═══════════════════════════════════════════
   Name              │ template0
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   appdev# \l+
   appdev# \set ECHO_HIDDEN false
   appdev# select datname,
   appdev-# pg_database_size(datname) as bytes
   appdev-# from pg_database
   appdev-# order by bytes desc;
   datname  │  bytes   
   ═══════════╪══════════
   appdev    │ 25596387
   template1 │  7602703
   postgres  │  7537167
   template0 │  7537167
   (4 rows)
   #+end_src

   For edit last SQL query in visual editor:
   #+begin_src
   appdev# select datname,
   pg_database_size(datname) as bytes
   from pg_database
   order by bytes desc;
   datname  │  bytes   
   ═══════════╪══════════
   appdev    │ 25596387
   template1 │  7602703
   postgres  │  7537167
   template0 │  7537167
   (4 rows)

   appdev# \e
   select datname,
   pg_database_size(datname) as bytes
   from pg_database
   order by bytes desc;
   "/tmp/psql.edit.334.sql" 4L, 89B
   1,1           All
   #+end_src
* 7. SQL is Code
  We approached a good way to have SQL queries as .sql files in code base.

  Now that SQL is actually code in application's source tree, we need to apply
  the ssame methodoloy that you're used to: set a minimum lefel of expected
  quality,
  code comments, consistent naming, unit testing and code revision systems.
** SQL style guidelines 
   A few examples of bad and good style:
   - 1.
     #+begin_src sql
     SELECT title, name FROM albume LEFT JOIN track USING(albumid) WHERE
     albumid = 1 ORDER BY 2
     #+end_src
     Query is using the old habit of all-caps SQL keywords. \\  
     We now have color screens and syntax highlighting and we don't write
     all-caps
     code anymore
   - 
     #+begin_src sql
     select title, name
     from album left join track using(albumid)
     where albumid=1
     order by 2;
     #+end_src
     Now it's quite a bit easier to understand the structure of this query at a
     glance and to realize
     that it is indeed a very basic SQL statement.


* 8. Indexing Strategy
** Indexing for Constraints 
   PostgreSQL provides a rich set of tools for developers to manage concurrent
   access to data.
   Internally, data consistency is mantained by using a multiversion model
   (Multiversion Concurrency Control,
   MVCC).
   This meands that each SQL statement sees a snapshot of data (a database
   version) as it was some time
   ago, regardless of the current state of the underlying data.

   Example with two transactions /t1/ and /t2/ happening in parallel:
   #+begin_src
   t1> insert into test(id) values(1);
   t2> insert into test(id) values(1);
   #+end_src
   One of them has to be refused, because they are conflicting with the one
   another.
   PostgreSQL knows how to do that, and thee implementation relies on the
   internal code being
   able to acceess the indexes in a non-MVCC compliant way: the internal code
   of PostgreSQL knows
   what the in-flight non-committed thansactions are doing.

   The way the internals of PostgreSQL solve this problem is by relying on its
   index data structure in a
   non-MVCC compliant way, and this capability is not visible to SQL level
   users.

   So when you declare a /unique/ constraint, a /primary key/ constrain or an
   /exclusion constraint/
   PostgreSQL creates an index for you:
   #+begin_src
   appdev# create table test(id integer unique);
   CREATE TABLE
   appdev# \d test
                  Table "chinook.test"
   Column │  Type   │ Collation │ Nullable │ Default 
   ════════╪═════════╪═══════════╪══════════╪═════════
   id     │ integer │           │          │ 
   Indexes:
      "test_id_key" UNIQUE CONSTRAINT, btree (id)
   #+end_src

** Indexing for Queries

   PostgreSQL automatically creates only those indexes that are needed for the
   system
   to behave correctly.
   Any and all other indexes are to be defined by the *application developers*
   when they need a faster access method to some tuples.

   An index cannot alter the result of query. An index only provides another
   access
   method to the data, one that is faster than a sequential scan in most cases.
   Query semantics and result set don't depend on indexes.

   Implementing a user story (or a business case) with the help of SQL queries
   is
   the job of the developer.
   As the authore of the SQL statements, the developer also should be
   responsible for
   choosing which indexes are needed to support their queries.
** PostgreSQL Index Access Methods
   An /access method/ is a generic algorithm with a clean API that can be
   imlemented
   for compatible data types.

   #+begin_quote
   PostgreSQL provides several index types: B-tree, Hash, GiST, SP-GiST, GIN
   and BRIN.
   Each index type uses a different algorithm that is best suited to different
   types of queries.
   By default, the *CREATE INDEX* command creates B-tree indexes, which fit the
   most common
   situations.
   #+end_quote

   Each index access method has been designed to solve specific use case:
   - /B-Tree/, or balanced tree \\  
     Balanced indexes are the most common used, by a long shot, because they
     are very
     efficient and provide an algorithm that applies to most cases.
   - /GiST/, or generalized search tree \\  
     Its implementation in PostgreSQL allows support for 2-dimensional data
     types such
     as the geometry /point/ or the /ranges/ data types. 
   - /SP-GiST/, or spaced partitioned gist \\  
     /SP-GiST/ indexes are the only PostgreSQL index access method
     implementation that
     support non-balanced disk-based data structures, such as quadtrees, k-d
     trees, and
     radix trees (tries).
     This is useful when you want to index 2-dimensional data with very
     different densities.
   - /GIN/, or generalized inverted index \\  
     /GIN/ is designed for handling cases where the items to be indexed are
     composite values,
     and the queries to be handled by the index need to search for element
     values that appear
     within the composite items. \\  
     /GIN/ indexes are "inverted indexes" which are appropriate for data values
     that contain
     multiple component values, such as arrays. An inverted index contains a separate entry
     for each component value.
   - /BRIN/, or block range indexes \\  
     Store summaries about the values stored in consecutive physical block
     ranges of a table.
     Can support many different indexing strategies, and the particular
     operators with which a
     BRIN index can be used vary depending on the indexing strategy.
     For data types that have a linear sort order, the indexed data corresponds
     to the minimum
     and maximum values of the values in the column for each block range.
   - /Hash/ \\  
     Can only handle simple equality comparisons. 
   - /Bloom filters/ \\  
     Is a space-efficient data structure that is used to test whether an
     element is a member
     of a set. \\  
     This type of index is most useful when a table has many attributes and
     queries test arbitrary
     combinations of them.
     A traditional B-tree index is faster that a Bloom index, but it can
     require many B-tree
     indexes to support all possible queries where one needs only a signle
     Bloom index. \\  
     Also, they are useful when the queries themselves are referencing most or
     all of those columns
     in equality comparisons.
** Adding Indexes 
   Not every query needs to be that fast, and the requirements are mostly user
   defined.

   Indexing needs analysis by listing every query that averages out to more
   that 10 milliseconds,
   or some other sensible threshold for application.
   The only way to understand where time is spent in a query is by using the
   *EXPLAIN* command
   and reviewing the /query plan/. \\  
   From the documentation:
   #+begin_quote
   You can use the EXPLAIN command to see what query plan the planner creates
   for any query.
   Plan-reading is an art that requries some experience to master ...
   #+end_quote
* 9. An Interview with Yohann Gabory 
  [[https://media.s-bol.com/OPj3zXW3mlvE/E8lg9RY/989x1200.jpg]]

* 10. Get Some Data
  We need f1db now:
  #+begin_src bash
  $ curl -fsSL
  "https://github.com/edpyt/TAOP-sql/raw/3a7c45887a4223a730a7659cb2325990d0696cfd/TheArtOfPostgreSQL-database-sql/f1db.sql"
  \
     | docker exec -i the_art_of_postgresql_book \
     psql -U postgres -d appdev 
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -c "ALTER ROLE
  postgres SET search_path TO f1db, public;"
  #+end_src

* 12. Queries, DML, DDL, TCL, DCL
  - DML Stands for /data manipulation language/ and it covers /insert/, /update/ and /delete/ statements,
    which are used to input data into the system
  - DDL Stands for /data definition language/ and it covers /create/, /alter/
    and /drop/ statements, which
    are used to define on-disk data structures where to hold the data, and also
    their constraints
  - TCL Stands for /transaction control language/ and includes /begin/ and
    /commit/ statements, and also
    /rollback/, /start transaction/ and /set transaction/ commands.  \\  
    It also includes the less well-known /savepoint/, /release savepoint/, and
    /rollback/ to /savepoint/
    commands, and the two-phase commit protocol with /prepare commit/, /commit
    prepared/ and
    /rollback prepared/ commands.
  - DCL Stands for /data control language/ and is covered with the statements
    /grant/ and /revoke/
  - PostgreSQL maintenance commands such as /vacuum/, /analyze/, /cluster/
  - There further commands that are provided by PostgreSQL such as /prepare/
    and /execute/,
    /explain/, /listen/ and /notify/, /lock/ and /set/, and some more.
* 13. Select, From, Where 

** Projection (output): Select
   The SQL /select/ clause introduces the list of output columns.
   This is called a /projection/.

   Adding a column to the /select/ list might have involve a lot of work, such
   as:
   - Fetching data on-disk
   - Possibly uncompressing data that is stored externally to the main table
     on-disk
     structure, and loading those uncompressed bytes into the memory of the
     database
     server
   - Sending the data back over the network back to the client application
*** Select Star 
    There's another reason to refrain from using the /select star/ notation in
    application's code:
    if you ever change the source relation definitions, then the same query now
    has a different
    result set data structure, and you might have to reflect that change in the
    application's in-memory
    data structures.
*** Select Computed Values and Aliases
    In the /SELECT/ clause it is possible to return computed values and to
    rename colums.  \\  
    Example:
    #+begin_src sql
    select code,
       format('%s %s', forename, surname) as fullname,
       forename,
       surname
    from drivers;
    #+end_src

    The SQL standard gives a concatenation operator named || and we could
    achieve the same result
    with a standard conforming query:
    #+begin_src sql
    select code,
       forename || ' ' || surname as fullname,
       forename,
       surname
    from drivers;
    #+end_src

*** PostgreSQL Processing Functions
    #+begin_src sql
    select date::date,
       extract('isodow' from date) as dow,
       to_char(date, 'dy') as day,
       extract('isoyear' from date) as "iso year",
       extract('week' from date) as week,
       extract('day' from
       (date + interval  '2 month - 1 day')
       ) as feb,
       extract('year' from date) as year,
       extract('day' from 
       (date + interval '2 month - 1 day')
       ) = 29
       as leap
    from generate_series(date '2000-01-01',
    date '2010-01-01',
    interval '1 year')
    as t(date);
    #+end_src

    The /generate_series()/ function returns a set of items, here all the dates
    of the
    first day of the years from the 2000s.

    Here's an extract from the PostgreSQL documentation about ISO years and week numbers:
    #+begin_quote
    By definition, ISO weeks start on Mondays and the first week of year
    contains January 4
    of that year.
    In other words, the first Thursday of a year is in week 1 of that year.
    #+end_quote

    It is very easy to do complex computations on dates in PostgreSQL, and that includes taking
    care of time zones too.
    Don't even think about coding such processing yourself, as it's full of oddities.

** Data sources: From
   The SQL /from/ clause introduces the data sources used in the query, and supports declaring
   how those different sources relate to each other.
   In the most basic form, our query is reading a data set from a single table:
   #+begin_src sql
   select code, driverref, forename, surname
   from drivers;
   #+end_src

   To find the all-time top three drivers, we fetch how many times each driver
   had /position/
   = 1 in the result table:
   #+begin_src sql
   select code, forename, surname, count(*) as wins
   from drivers
   join results using(driverid)
   where position = 1
   group by driverid
   order by wins desc
   limit 3;
   #+end_src
   The query uses an /inner join/ in between the /drivers/ and the /results/ table.
   In both those tables, there is a /driverid/ column that we can use as a
   lookup
   reference to associate data in between the two tables.

** Understanding Joins
*** PostgreSQL docs 

    From the PostgreSQL documentation - [[https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-FROM][the FROM clause]]:

    The *FROM* clause derives a table from one or more other tables given in a
    comman-separated
    table reference list.
    #+begin_example
    FROM table_reference [, table_reference [, ...]]
    #+end_example
    A /table_reference/ can be a table name, or a derived table such as a subquery, a *JOIN* construct,
    or complex combinations of these.

    A joined table is a table derived from two other tables according to the
    rules of the particular
    join type.
    Inner, outer, and cross-joins are available. The general syntax of a joined
    table is
    #+begin_example
    T1 join_type T2 [ join_condition ]
    #+end_example

    *Join Types*:
    - /Cross join/ \\  
      #+begin_example
      T1 CROSS JOIN T2
      #+end_example

      For every possible combination of rows from *T1* and *T2*, the joined table will contain a row
      consisting of all columns in *T1* followed by all columns in *T2*.
      If the tables have N and M rows respectively, the joined table will have
      N * M rows.

      *FROM T1 CROSS JOIN T2* is equivalent to *FROM T1 INNER JOIN T2 ON TRUE*
    - /Qualified joins/ \\  
      #+begin_example
      T1 { [INNER] | { LEFT | RIGHT | FULL } [OUTER] } JOIN T2 ON boolean_expression
      T1 { [INNER] | { LEFT | RIGHT | FULL } [OUTER] } JOIN T2 USING ( join column list )
      T1 NATURAL { [INNER] | { LEFT | RIGHT | FULL } [OUTER] } JOIN T2
      #+end_example
      The words *INNER* and *OUTER* are optional in all forms. *INNER* is the default;
      *LEFT*, *RIGHT*, and *FULL* imply an outer join.

      The /join condition/ is specified in the *ON* or *USING* clause, or
      implicitly by
      the word *NATURAL*.

      The possible types of qualified join are:
      * *INNER JOIN* \\  
        For each row R1 of T1, the joined table has a row for each row in T2
        that satisfies
        the join condition with R1
      * *LEFT OUTER JOIN* \\  
        First, an inner join is performed.
        Then, for each row in T1 that does not satisfy the join condition with
        any row in T2,
        a joined row is added with null values in columns of T2.
        Thus, the joined table always has at least one row for each row in T1.
      * *RIGHT OUTER JOIN* \\  
        First, an inner is performed.
        Then, for each row in T2 that does not satisfy the join condition with any row in T1,
        a joined row is added with null values in columns of T1.
        This is the converse of a left join, the result table will always have a row for each
        row in T2.
      * *FULL OUTER JOIN* \\  
        First, an inner join is performed.
        Then, for each row in T1 that does not satisfy the join condition with
        any row in T2,
        a joined row is added with null values in columns of T2.
        Also, for each row of T2 that does not satisfy the join condition with any row in T1,
        a joined row with null values in the columns of T1 is added.

      The *ON* clause is the most general kind of join condition: it takes a
      Boolean value
      expression of the same kind as is used in a *WHERE* clause. A pair of
      rows from *T1*
      and *T2* match if the *ON* expression evaluates to true.

      The *USING* clause is a shorthand that allows you to take advantage of
      the specific situation
      where both sides of the join use the same name for the joining column(s).
      For example, joining *T1* and *T2* with *USING(a,b)* produces the join
      condition
      *ON T1.a = T2.a AND T1.b = T2.b*

      Finally, *NATURAL* is a shorthand form of *USING*: it forms a *USING*
      list consisting of 
      all column names that appear in both input tables.
      As with *USING*, these columns appear only once in the output table. If
      there are no
      common column names, *NATURAL JOIN* behaves like *CROSS JOIN*.
**** Example with joins
     Assume we have tables:
     - t1 \\  
       | num | name |
       |-----+------|
       | 1   | a    |
       | 2   | b    |
       | 3   | c    |
     - t2 \\  
       | num | value |
       |-----+-------|
       | 1   | xxx   |
       | 3   | yyy   |
       | 5   | zzz   |

     Then we got the following results for the various join:
     * 
       #+begin_example
       => SELECT * FROM t1 CROSS JOIN t2;
       #+end_example
       | num | name | num | value |
       |-----+------+-----+-------|
       | 1   | a    | 1   | xxx   |
       | 1   | a    | 3   | yyy   |
       | 1   | a    | 5   | zzz   |
       | 2   | b    | 1   | xxx   |
       | 2   | b    | 3   | yyy   |
       | 2   | b    | 5   | zzz   |
       | 3   | c    | 1   | xxx   |
       | 3   | c    | 3   | yyy   |
       | 3   | c    | 5   | zzz   |
     * 
       #+begin_example
       => SELECT * FROM t1 INNER JOIN t2 ON t1.num = t2.num;
       #+end_example
       | num | name | num | value |
       |-----+------+-----+-------|
       | 1   | a    | 1   | xxx   |
       | 3   | c    | 3   | yyy   |
     * 
       #+begin_example
       => SELECT * FROM t1 INNER JOIN t2 USING (num);
       #+end_example
       | num | name | value |
       |-----+------+-------|
       | 1   | a    | xxx   |
       | 3   | c    | yyy   |
     * 
       #+begin_example
       => SELECT * FROM t1 NATURAL INNER JOIN t2;
       #+end_example
       | num | name | value |
       |-----+------+-------|
       | 1   | a    | xxx   |
       | 3   | c    | yyy   |
     * 
       #+begin_example
       => SELECT * FROM t1 LEFT JOIN t2 ON t1.num = t2.num;
       #+end_example
       | num | name | num | value |
       |-----+------+-----+-------|
       | 1   | a    | 1   | xxx   |
       | 2   | b    |     |       |
       | 3   | c    | 3   | yyy   |
     * 
       #+begin_example
       => SELECT * FROM t1 LEFT JOIN t2 USING (num);
       #+end_example
       | num | name | value |
       |-----+------+-------|
       | 1   | a    | xxx   |
       | 2   | b    |       |
       | 3   | c    | yyy   |
     * 
       #+begin_example
       => SELECT * FROM t1 RIGHT JOIN t2 ON t1.num = t2.num;
       #+end_example
       | num | name | num | value |
       |-----+------+-----+-------|
       | 1   | a    | 1   | xxx   |
       | 3   | c    | 3   | yyy   |
       |     |      | 5   | zzz   |
     * 
       #+begin_example
       => SELECT * FROM t1 FULL JOIN t2 ON t1.num = t2.num;
       #+end_example
       | num | name | num | value |
       |-----+------+-----+-------|
       | 1   | a    | 1   | xxx   |
       | 2   | b    |     |       |
       | 3   | c    | 3   | yyy   |
       |     |      | 5   | zzz   |
*** f1db
    Now that we know how to easily fetch the winner of a race, it is possible to also
    to display all the races from a quarter with their winner:
    #+begin_src sql
    \set beginning '2017-04-01'
    \set months 3
    select date, name, drivers.surname as winner
    from races
    left join results
    on results.raceid = races.raceid
    and results.position = 1
    left join drivers using(driverid)
    where date >= date :'beginning'
    and date < date :'beginning'
    months * interval '1 month';
    #+end_src
    And we get the following result:
    | date       | name                  | winner    |
    |------------+-----------------------+-----------|
    | 2017-04-09 | Chinese Grand Prix    | Hamilton  |
    | 2017-04-16 | Bahrain Grand Prix    | Vettel    |
    | 2017-04-30 | Russian Grand Prix    | Bottas    |
    | 2017-05-14 | Spanish Grand Prix    | Hamilton  |
    | 2017-05-28 | Monaco Grand Prix     | Vettel    |
    | 2017-06-11 | Canadian Grand Prix   | Hamilton  |
    | 2017-06-25 | Azerbaijan Grand Prix | Ricciardo |
    (7 rows)


** Restrictions: Where
   This clause acts as a filter for the query: when the filter evaluates to true then we
   keep the row in the result set and when the filter evaluates to false we skip that row.

   We need try to keep the /where/ clauses as simple as possible for PostgreSQL in order
   to be able to use our indexes to solve the data filtering expressions of our queries.

   It is possible in the /where/ clause to use a subquery, and that's quite common to use
   when implementing the /anti-join/ pattern thanks to the special feature /not exists/.

   An /anti-join/ is meant to keep only the rows that fail a test.
   If we want to list the drivers that where unlucky enough to not finish a single race
   in which they participated, then we can filter out those who did finish. 
   We know that a driver finished because their /position/ is filled in the results table:
   it is /not null/

   If we translate the previous sentence into the SQL language, here's what we have:
   #+begin_src sql
   \set season 'date ''1978-01-01'''

   select forename,
   surname,
   constructors.name as constructor,
   count(*) as races,
   count(distinct status) as resons
   from drivers
   join results using(driverid)
   join races using(raceid)
   join status using(statusid)
   join constructors using(constructorid)
   where date >= :season
   and date < :season + interval '1 year'
   and not exists
   (
   select 1
   from results r
   where position is not null
   and r.driverid = drivers.driverid
   and r.resultid = results.resultid
   )
   group by constructors.name, driverid
   order by count(*) desc;
   #+end_src
   | forename     | surname   | constructor | races | resons |
   |--------------+-----------+-------------+-------+--------|
   | Arturo       | Merzario  | Merzario    | 16    | 8      |
   | Hector       | Rebaque   | Team Lotus  | 12    | 7      |
   | Hans-Joachim | Stuck     | Shadow      | 12    | 6      |
   | Rupert       | Keegan    | Surtees     | 12    | 6      |
   | James        | Hunt      | McLaren     | 10    | 6      |
   | Clay         | Regazzoni | Shadow      | 10    | 5      |
   (10 rows)

* 14. Order By, Limit, No Offset
** Ordering with Order By 
   The SQL /ORDER BY/ clause is pretty well-known because SQL doesn't guarantee
   any ordering of the result set of any query except when you use the /order by/ clause.

   Example:
   #+begin_src sql
   select year, url
   from seasons
   order by year desc
   limit 3;
   #+end_src
   | year | url                                                   |
   |------+-------------------------------------------------------|
   | 2017 | https://en.wikipedia.org/wiki/2017_Formula_One_season |
   | 2016 | https://en.wikipedia.org/wiki/2016_Formula_One_season |
   | 2015 | http://en.wikipedia.org/wiki/2015_Formula_One_season  |

   What is more interesting about it is the /explain plan/ of the query, where we see
   PostgreSQL follows the primary key index of the table in a backward direction
   in order to return our three most recent entries.
   #+begin_src sql
   explain (costs off)
   select year, url
   from seasons
   order by year desc
   limit 3;
   #+end_src
   | QUERY PLAN                                     |
   |------------------------------------------------|
   | Limit                                          |
   | -> Index Scan Backward using idx_.* on seasons |
** kNN Ordering and GiST indexes
   Another use case for /order by/ is to implement /k nearest neighbours/.

   Let's find out the ten nearest circuits to Paris, France, which is at longitude
   2.349014 and latitude 48.864716. That's a kNN search with k = 10:
   #+begin_src sql
   select name, location, country
   from circuits
   order by point(lng, lat) <-> point(2.349014, 48.864716)
   limit 10;
   #+end_src

   #+begin_example
                name              │     location     │ country 
   ═══════════════════════════════╪══════════════════╪═════════
    Rouen-Les-Essarts             │ Rouen            │ France
    Reims-Gueux                   │ Reims            │ France
    Circuit de Nevers Magny-Cours │ Magny Cours      │ France
    Le Mans                       │ Le Mans          │ France
    Nivelles-Baulers              │ Brussels         │ Belgium
    Dijon-Prenois                 │ Dijon            │ France
    Charade Circuit               │ Clermont-Ferrand │ France
    Brands Hatch                  │ Kent             │ UK
    Zolder                        │ Heusden-Zolder   │ Belgium
    Circuit de Spa-Francorchamps  │ Spa              │ Belgium
   (10 rows)
   #+end_example

   The /point/ datatype is a very useful PostgreSQL addition. For a proper PostgreSQL
   experience, we can have a location column of point type in our circuits table and 
   index it using *GiST*:
   #+begin_src sql
   begin;

   alter table f1db.circuits add column position point;
   update f1db.circuits set position = point(lng,lat);
   create index on f1db. circuits using gist(position);

   commit;
   #+end_src

   Query plan now that we have a *GiST* index defined:
   #+begin_src sql
   explain (costs off, buffers, analyze)
   select name, location, country
   from circuits
   order by position <-> point(2.349014, 48.864716)
   limit 10;
   #+end_src

   #+begin_example
                                                    QUERY PLAN                                              
   ══════════════════════════════════════════════════════════════════════════════════════════════════════
    Limit (actual time=0.029..0.034 rows=10 loops=1)
      Buffers: shared hit=8
      ->  Index Scan using circuits_position_idx2 on circuits (actual time=0.029..0.033 rows=10 loops=1)
            Order By: ("position" <-> '(2.349014,48.864716)'::point)
            Buffers: shared hit=8
    Planning:
      Buffers: shared hit=51
    Planning Time: 0.236 ms
    Execution Time: 0.051 ms
   (9 rows)
   #+end_example

** No Offset, and how to implement pagination
   Using the /offset/ clause is very bad of query performances ([[https://use-the-index-luke.com/sql/partial-results/fetch-next-page][Paging Through Results]]).

   The /offset/ clause is going to cause SQL query plan to read all the result anyway and then
   discard most of it until reaching the /offset/ count.

   The proper way to implement pagination is to use index lookups, you cna do that with the /row()/
   construct.

   Example:
   #+begin_src sql
   select lap, drivers.code, position,
      milliseconds * interval '1ms' as laptime
   from laptimes
   join drivers using(driverid)
   where raceid = 972
   order by lap, position
   fetch first 3 rows only;
   #+end_src
   #+begin_example
    lap │ code │ position │   laptime    
   ═════╪══════╪══════════╪══════════════
      1 │ BOT  │        1 │ 00:02:05.192
      1 │ VET  │        2 │ 00:02:07.101
      1 │ RAI  │        3 │ 00:02:10.53
   (3 rows)
   #+end_example

   Next query shows the next page of results:
   #+begin_src sql
   select lap, drivers.code, position,
      milliseconds * interval '1ms' as laptime
   from laptimes
   join drivers using(driverid)
   where raceid = 972
   and row(lap,position) > (1,3)
   order by lap, position
   fetch first 3 rows only;
   #+end_src
   #+begin_example
    lap │ code │ position │   laptime    
   ═════╪══════╪══════════╪══════════════
      1 │ HAM  │        4 │ 00:02:11.18
      1 │ VER  │        5 │ 00:02:12.202
      1 │ MAS  │        6 │ 00:02:13.501
   (3 rows)
   #+end_example

* 15. Group By, Having, With, Union All 

** Aggregates (aka Map/Reduce): Group By 
   The /group by/ clause introduces aggregates in SQL, and allows implementing much
   the same thing as /map//reduce/ in other systems: map your data into different
   groups, and in each group reduce the data set to a single value.

   As a first example we can count how many races have been run in each decade:
   #+begin_src sql
   select extract('year'
      from date_trunc('decade', date))
      as decade,
      count(*)
   from races
   group by decade
   order by decade;
   #+end_src
   #+begin_example
    decade │ count 
   ════════╪═══════
      1950 │    84
      1960 │   100
      1970 │   144
      1980 │   156
      1990 │   162
      2000 │   174
      2010 │   156
   (7 rows)
   #+end_example

   The difference between each decade is easy to compute thanks to /window function/.
   Let's have a preview:
   #+begin_src sql
   with races_per_decade
      as (
      select extract('year'
         from date_trunc('decade', date))
         as decade,
         count(*) as nbraces
      from races
      group by decade
      order by decade
   )
   select decade, nbraces,
      case
         when lag(nbraces,1)
         over(order by decade) is null
         then ''

         when nbraces - lag(nbraces, 1)
         over(order by decade) < 0
         then format('-3%s',
         lag(nbraces, 1)
         over(order by decade)
         - nbraces)

         else format('+%3s',
         nbraces - lag(nbraces,1)https://duckduckgo.com/?t=ffab&q=%D0%BC%D0%B0%D1%87%D1%82%D0%B0&ia=web
         over(order by decade))

      end as evolution
   from races_per_decade;
   #+end_src
   #+begin_example
    decade │ nbraces │ evolution 
   ════════╪═════════╪═══════════
      1950 │      84 │ 
      1960 │     100 │ + 16
      1970 │     144 │ + 44
      1980 │     156 │ + 12
      1990 │     162 │ +  6
      2000 │     174 │ + 12
      2010 │     156 │ - 18
   (7 rows)
   #+end_example

   Search for all drivers who failed to finish any single race they participated in over
   their whole career:
   #+begin_src sql
   with counts as
      (
      select driverid, forename, surname,
         count(*) as races,
         bool_and(position is null) as never_finished
      from drivers
      join results using(driverid)
      join races using(raceid)
      group by driverid
   )
   select driverid, forename, surname, races
   from counts
   where never_finished
   order by races desc;
   #+end_src

   Now, we can find out if some seasons were less lucky than others on that basis and search
   for drivers who didn't finish a single race thay participated into, per season:
   #+begin_src sql
   with counts as
      (
      select date_trunc('year', date) as year,
         count(*) filter(where position is null) as outs,
         bool_and(position is null) as never_finished
      from drivers
      join results using(driverid)
      join races using(raceid)
      group by date_trunc('year', date), driverid
   )
   select extract(year from year) as season,
      sum(outs) as "#times any driver didn't finish a race"
   from counts
   where never_finished
   group by season
   order by sum(outs) desc
   limit 5;
   #+end_src
** Common Table Expressions: With
   Most dangerous seasons in terms of accidents:
   #+begin_src sql
   select extract(year from races.date) as season,
      count(*)
      filter(where status = 'Accident') as accidents
   from results
   join status using(statusid)
   join races using(raceid)
   group by season
   order by accidents desc
   limit 5;
   #+end_src

   #+begin_example
    season │ accidents 
   ════════╪═══════════
      1977 │        60
      1975 │        54
      1976 │        48
      1978 │        48
      1985 │        36
   (5 rows)
   #+end_example

   It seems the most dangerous seasons of all time are clustered at the end
   of the 70s and the beginning of the 80s, so zoom on this period:
   #+begin_src sql
   with accidents as
      (
      select extract(year from races.date) as season,
         count(*) as participants,
         count(*) filter(where status = 'Accident') as accidents
      from results
      join status using(statusid)
      join races using(raceid)
      group by season
   )
   select season,
      round(100.0 * accidents / participants, 2) as pct,
      repeat(
      text '🟦',
      ceil(100*accidents/participants)::int
      )
      as bar
   from accidents
   where season between 1974 and 1990
   order by season;
   #+end_src

   #+begin_example
    season │  pct  │             bar              
   ════════╪═══════╪══════════════════════════════
      1974 │  3.67 │ 🟦🟦🟦
      1975 │ 14.88 │ 🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦
      1976 │ 11.06 │ 🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦
      1977 │ 12.58 │ 🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦
      1978 │ 10.19 │ 🟦🟦🟦🟦🟦🟦🟦🟦🟦🟦
      1979 │  7.20 │ 🟦🟦🟦🟦🟦🟦🟦
      1980 │  7.83 │ 🟦🟦🟦🟦🟦🟦🟦
      1981 │  3.56 │ 🟦🟦🟦
      1982 │  0.86 │ 
      1983 │  0.00 │ 
      1984 │  5.58 │ 🟦🟦🟦🟦🟦
      1985 │  8.87 │ 🟦🟦🟦🟦🟦🟦🟦🟦
      1986 │  6.07 │ 🟦🟦🟦🟦🟦🟦
      1987 │  5.97 │ 🟦🟦🟦🟦🟦
      1988 │  0.61 │ 
      1989 │  0.81 │ 
      1990 │  1.29 │ 🟦
   (17 rows)
   #+end_example

   To compute the driver/constructor who won the most points in total in the races that year:
   #+begin_src sql
   with points as
      (
      select year as season, driverid, constructorid,
         sum(points) as points
      from results join races using(raceid)
      group by grouping sets((year, driverid),
      (year, constructorid))
      having sum(points) > 0
      order by season, points desc
   ),
   tops as
      (
      select season,
         max(points) filter(where driverid is null) as ctops,
         max(points) filter(where constructorid is null) as dtops
      from points
      group by season
      order by season, dtops, ctops
   ),
   champs as
      (
      select tops.season,
         champ_driver.driverid,
         champ_driver.points,
         champ_constructor.constructorid,
         champ_constructor.points
      from tops
      join points as champ_driver
      on champ_driver.season = tops.season
      and champ_driver.constructorid is null
      and champ_driver.points = tops.dtops
      join points as champ_constructor
      on champ_constructor.season = tops.season
      and champ_constructor.driverid is null
      and champ_constructor.points = tops.ctops
   )
   select season,
      format('%s %s', drivers.forename, drivers.surname)
      as "Driver's Champion",
      constructors.name as "Constructor's champion"
   from champs
   join drivers using(driverid)
   join constructors using(constructorid)
   order by season;
   #+end_src

   #+begin_example
    season │ Driver's Champion  │ Constructor's champion 
   ════════╪════════════════════╪════════════════════════
      1985 │ Alain Prost        │ McLaren
      1986 │ Alain Prost        │ Williams
      1987 │ Nelson Piquet      │ Williams
      1988 │ Alain Prost        │ McLaren
      ...
      2015 │ Lewis Hamilton     │ Mercedes
      2016 │ Nico Rosberg       │ Mercedes
      2017 │ Lewis Hamilton     │ Mercedes
   (68 rows)
   #+end_example

** Distinct On
   PostgreSQL distint clause documentation:
   #+begin_quote
   SELECT DISTINCT ON (expression [,...]) keeps only the first row of each set of rows
   where the given expressions evaluate to equal.
   The DISTINCT ON expressions are interpreted using the same rules as for ORDER BY.
   Note that the "first row" of each set is unpredictable unless ORDER BY is used
   to ensure that the desired row appears first.
   #+end_quote

   So it is possible to return the list of drivers who ever won a race in the whole
   Formula One history with the following query:
   #+begin_src sql
   select distinct on (driverid)
      forename, surname
   from results
   join drivers using(driverid)
   where position = 1;
   #+end_src
   There 107 of them

   The classic way to have a single result per driver in SQL would be to aggregate
   over them, creating a group per driver:
   #+begin_src sql
   select forename, surname
   from results join drivers using(driverid)
   where position = 1
   group by drivers.driverid;
   #+end_src

** Result Sets Operations
   SQL also includes set operations for combining queries results sets into a single
   one.

   The set operations are /union/, /intersect/, and /except/.
   As expected with /union/ you can assemble a result set from the result of several queries:
   #+begin_src sql
   (
   select raceid,
      'driver' as type,
      format('%s %s', drivers.forename, drivers.surname)
      as name,
      driverstandings.points
   from driverstandings
   join drivers using(driverid)
   where raceid = 972
   and points > 0
   )
   union all
   (
   select raceid,
      'constructor' as type,
      constructors.name as name,
      constructorstandings.points
   from constructorstandings
   join constructors using(constructorid)
   where raceid = 972
   and points > 0
   )
   order by points desc;
   #+end_src

   #+begin_example
    raceid │    type     │       name       │ points 
   ════════╪═════════════╪══════════════════╪════════
       972 │ constructor │ Mercedes         │    136
       972 │ constructor │ Ferrari          │    135
       972 │ driver      │ Sebastian Vettel │     86
       972 │ driver      │ Lewis Hamilton   │     73
       972 │ driver      │ Valtteri Bottas  │     63
       972 │ constructor │ Red Bull         │     57
       972 │ driver      │ Kimi Räikkönen   │     49
       972 │ driver      │ Max Verstappen   │     35
       972 │ constructor │ Force India      │     31
       972 │ driver      │ Daniel Ricciardo │     22
       972 │ driver      │ Sergio Pérez     │     22
       972 │ constructor │ Williams         │     18
       972 │ driver      │ Felipe Massa     │     18
       972 │ constructor │ Toro Rosso       │     13
       972 │ driver      │ Carlos Sainz     │     11
       972 │ driver      │ Esteban Ocon     │      9
       972 │ constructor │ Haas F1 Team     │      8
       972 │ driver      │ Nico Hülkenberg  │      6
       972 │ constructor │ Renault          │      6
       972 │ driver      │ Romain Grosjean  │      4
       972 │ driver      │ Kevin Magnussen  │      4
       972 │ driver      │ Daniil Kvyat     │      2
   (22 rows)
   #+end_example

* 16. Understanding Nulls
