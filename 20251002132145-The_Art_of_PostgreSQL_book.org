:PROPERTIES:
:ID: 0ec0df6d-941f-40ff-9dee-bc56c521e53b
:BOOK_URL: [[https://storage.sbg.cloud.ovh.net/v1/AUTH_e5524010dbdf45ccb5cdac68b254c4f7/TAOP/TAOP-Volume-1.pdf]]
:END:
#+TITLE: The Art of PostgreSQL book

* 1. Structured Query Language (SQL)
  RDBMS (Relational DataBase Management System) and SQL are forcing developers to think in terms of data structure,
  and to declare both the data structure and the data set we want to obain
  via our queries.

  #+begin_quote
  Bad programmers worry about the code. \\
  Good programmers worry about data structures and their relationships.

  --- Linus Torvalds
  #+end_quote
** Some of the Code is Written in SQL
   The current SQL standard is SQL:2016.

   If your application is already using the SQL programming language
   and SQL engine, then as a developer it's important to fully understand how 
   much can be achieved in SQL, and what service is implemented by this runtime
   dependency in your software architecture.

   SQL is a very powerful programming language, and it is a declarative one.
   It's a wonderful tool to master, and once used properly it allows one to reduce
   both code size and the development time for new features.

** A First Use Case
   Fetch NYSE /Excel/ file and load it into a PostgreSQL table.

   How file looks:
   #+begin_example
   2010  1/4/2010    1,425,504,460  4,628,115   $38.495.460,645
   2010  1/5/2010    1,754,011,760  5,394,016   $43.932.043,406
   2010  1/6/2010    1,655,507,953  5,494,460   $43.816.749,660
   #+end_example

   #+begin_src sql 
   begin;

   drop table if exists factbook;

   create table factbook
   (
      year int,
      date date,
      shares text,
      trades text,
      dollars text
   );

   \copy factbook from 'factbook.csv' with delimiter E'\t' null ''

   alter table factbook
   alter shares
   type bigint
   using replace(shares, ',', '')::bigint,

   alter trades
   type bigint
   using replace(trades, ',', '')::bigint,

   alter dollars
   type bigint
   using substring(replace(dollars, ',', '') from 2)::numeric;

   commit;
   #+end_src


** Application Code and SQL
   Query that list all entries we have in the month of February 2017:
   #+begin_src sql
   \set start '2017-02-01'

   select date, to_char(shares, '99G999G999G999') as shares, to_char(trades, '99G999G999') as trades, to_char(dollars, 'L99G999G999G999') as dollars
   from factbook
   where date >= date :'start'
   and date < date :'start' + interval '1 month'
   order by date;
   #+end_src

   Result of the query:
   | date       | shares        | trades    | dollars          |
   |------------+---------------+-----------+------------------|
   | 2017-02-01 | 1,161,001,502 | 5,217,859 | $ 44,660,060,305 |
   | (n rows)   |               |           |                  |

   Typical implementation of that expectation in Python
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg-binary",
   # ]
   # ///
   from datetime import date
   import sys
   import psycopg2
   import psycopg2.extras
   from calendar import Calendar

   CONNSTRING = "dbname=yesql application_name=factbook"


   def fetch_month_data(year: int, month: int) -> dict[date, tuple[str, str, str]]:
       date = "%d-%02d-01" % (year, month)
       sql = """
   select date, shared, trades, dollars
   from factbook
   where date >= date %s
   and date < date %s + interval '1 month'
   order by date;
   """
       pgconn = psycopg2.connect(CONNSTRING)
       curs = pgconn.cursor()
       curs.execute(sql, (date, date))

       return {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in curs.fetchall()
       }


   def list_book_for_month(year: int, month: int):
       data = fetch_month_data(year, month)
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades", "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" * 12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src

   Output when running the program
   #+begin_src sh
   $ uv run main.py 2017 2
   | day        | shares     | trades  | dollars     |
      |------------+------------+---------+-------------|
      | 2017-02-01 | 1161001502 | 5217859 | 44660060305 |
      | etc
   #+end_src
** A Word about SQL Injection
   [[https://imgs.xkcd.com/comics/exploits_of_a_mom.png]]

   It is advisable that read the documentation of current driver and understand how to send
   SQL query parameters separately from the main SQL query text;
   this is a reliable way to never have to worry about SQL injection problems ever again.

   In particular, never build a query string by concatenating query arguments directly
   into query strings, i.e. in the application client code.
   Never use library, ORM or another tooling that would do that.

   We were using the psycopg Python driver which is based on *libpq*. \\
   A lot of PostgreSQL application drivers are based on the libpq C driver, which
   implements the PostgreSQL protocols and is mantained alongside the main server's code.


** PostgreSQL protocol: server-side prepared statements
   Server-side Prepared Statements can be used in SQL thanks to the *PREPARE*
   and *EXECUTE* commands syntax:
   #+begin_src sql
   prepare foo as
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;

   -- And then execute the prepared statement with a parameter
   execute foo('2010-02-01') 
   #+end_src

   Remember: SQL injection happens when the SQL parser is fooled into beliving that
   a parameter string is in fact a SQL query, and then the SQL engine goes on and
   execute that SQL statement.

   *asyncpg* PostgreSQL driver that implements the PostgreSQL protocol itself, and uses
   server-side prepared statements.

   This example is now safe from SQL injection by design, because the server-side
   prepared statement protocol sends the query string and its arguments in separate protocol
   messages:
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "asyncpg",
   # ]
   # ///
   import sys
   import asyncio
   import asyncpg
   import datetime
   from calendar import Calendar

   CONNSTRING = "postgresql://appdev@localhost/appdev?application_name=factbook"


   async def fetch_month_data(year: int, month: int):
       date = datetime.date(year, month, 1)
       sql = """
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;
   """
       pgconn = await asyncpg.connect(CONNSTRING)
       stmt = await pgconn.prepare(sql)
       res = {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in stmt.fetch(date)
       }
       await pgconn.close()
       return res


   def list_book_for_month(year: int, month: int):
       data = asyncio.run(fetch_month_data(year, month))
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades", "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" * 12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src


** Back to Discovering SQL
   Now of course it's possible to implement the same expectations with a single SQL
   query, without any application code being spent on solving the problem:
   #+begin_src sql
   select cast(calendar.entry as date) as date,
      coalesce(shares, 0) as shares,
      coalesce(trades, 0) as trades,
      to_char(
      coalesce(dollars,0),
      'L99G999G999G999'
      ) as dollars
   from
   generate_series(date :'start',
   date :'start' + interval '1 month'
   - interval '1 day',
   interval '1 day'
   )
   as calendar(entry)
   left join factbook
   on factbook.date = calendar.entry
   order by date;
   #+end_src

   Here's the result of running this query:
   | date       | shares     | trades  | dollars          |
   |------------+------------+---------+------------------|
   | 2017-02-01 | 1161001502 | 5217859 | $ 44,660,060,305 |
   | 2017-02-02 | 1128144760 | 4586343 | $ 43,276,102,903 |
   | etc        |            |         |                  |

   Note that we replaced 60 lines of Python code with a simple enough SQL query
   Here, the Python is doing and /Hash Join Nested Loop/ where PostgreSQL picks a 
   /Merge Left Join/ over two ordered relations.

* 2. Software Architecture
  When designing your software architecture, you need to think about PostgreSQL
  not as /storage/ layer, but rather as a /concurrent data access service/.
  This service is capable of handling data processing.

** Why PostgreSQL? 
   That choice is down to several factors, all consequences of PostgreSQL
   truly being /the world's most advanced open source database/:
   - PostgreSQL is open source, available under a BSD like licence
     named the *PostgreSQL licence*
   - The PostgreSQL project is done completely in the open. \\
     The project goes as far as self-hosting all requirements in order
     to avoid being influenced by a particular company.
   - PostgreSQL releases a new major version about once a year, following a
     /when it's ready/ releaase cycle
   - The PostgreSQL design allows enhancing SQL in very advanced ways


* 3. Getting Ready to read this Book
  #+begin_src sh
  $ docker run -d -p 127.0.0.1:5432:5432 -e POSTGRES_PASSWORD="1234" --name the_art_of_postgresql_book postgres:16.9-alpine
  $ docker exec -it the_art_of_postgresql_book psql -U postgres
  postgres=# show server_version;
  server_version 
  ----------------
  16.9
  (1 row)
  #+end_src

  Need to import /Chinook/ database to PostgreSQL:
  #+begin_src sh
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -c "create database appdev"
  $ curl -fsSL "https://github.com/edpyt/TAOP-sql/raw/3a7c45887a4223a730a7659cb2325990d0696cfd/TheArtOfPostgreSQL-database-sql/chinook.sql" | docker exec -i the_art_of_postgresql_book psql -U postgres -d appdev 
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -d appdev -c "ALTER ROLE postgres SET search_path TO chinook;"
  #+end_src
* 4. Business Logic

** Every SQL query embeds some business logic
   Each and every and all SQL query contains some levels of business logic.

   Example:
   #+begin_src sql
   select name
   from track
   where albumid = 193
   order by trackid;
   #+end_src
   What business logic is embedded in that SQL statement?
   - The /select/ clause only mentions the /name/ column, and that's relevant
     to your application.
     In the situation in which your application runs this query, the business logic
     is only interested into the tracks names.
   - The /from/ clause only mentions the /track/ trable, somehow we decided that's
     all we need in this example, and that again is strongly tied to the logic being
     implemented
   - The /where/ clause restricts the data output to the /albumid/ 193 which again
     is a direct translation of our business logic, with the added information
     that the album we want now is the 193rd one and we're left to wonder how 
     we know about it
   - Finally, the /order by/ clause implements the idead that we want to display
     the track names in the order they appear on the disk.
     Not only that, it also incorporates the specific knowledge that the /trackid/
     column ordering is the same as the original disk ordering of the tracks.
** Business Logic Applies to Use Cases
   Display the list of albums from a given artist, each with its total duration.
   #+begin_src sql
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.name = 'Red Hot Chili Peppers'
   group by album
   order by album;
   #+end_src
   The output is:
   | album                 | duration     |
   |-----------------------+--------------|
   | Blood Sugar Sex Magik | 01:13:57.073 |
   | By The Way            | 01:08:49.951 |
   | Californication       | 00:56:25.461 |

   What we see here is a direct translation from the business case (or user story)
   into a SQL query. The SQL implementation uses joins and computations that are specific
   to both the data model and the use case we are solving.

   Another implementation could be done with several queries and the computation
   in the application's main code:
   1. Fetch the list of albums for the selected artist
   2. For each album, fetch the duration of every track in the album
   3. In the application, sum up the durations per album

   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg[binary]",
   # ]
   # ///
   from dataclasses import dataclass
   from typing import ClassVar, Self
   import psycopg
   import sys
   from datetime import timedelta

   from psycopg.rows import dict_row

   DEBUGSQL = False
   PGCONNSTRING = "host=localhost user=postgres password=1234 dbname=appdev"


   @dataclass
   class Model:
       tablename: ClassVar[str | None] = None
       columns: ClassVar[list[str]] = []

       @classmethod
       def buildsql(cls, pgconn: psycopg.Connection, **kwargs) -> str | None:
           if cls.tablename and kwargs:
               cols = ", ".join('"%s"' % c for c in cls.columns)
               qtab = '"%s"' % cls.tablename
               sql = "select %s from %s where" % (cols, qtab)
               for key in kwargs.keys():
                   sql += "\"%s\" = '%s'" % (key, kwargs[key])
                   if DEBUGSQL:
                       print(sql)
                   return sql

       @classmethod
       def fetchone(cls, pgconn: psycopg.Connection, **kwargs) -> Self | None:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if (result := curs.fetchone()) is not None:
                   return cls(*result.values())

       @classmethod
       def fetchall(cls, pgconn: psycopg.Connection, **kwargs) -> list[Self]:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if resultset := curs.fetchall():
                   return [cls(*result.values()) for result in resultset]
           return []


   @dataclass
   class Artist(Model):
       tablename = "artist"
       columns = ["artistid", "name"]

       id: int
       title: str


   @dataclass
   class Album(Model):
       tablename = "album"
       columns = ["albumid", "title"]

       id: int
       title: str
       duration: float | None = None


   @dataclass
   class Track(Model):
       tablename = "track"
       columns = ["trackid", "name", "milliseconds", "bytes", "unitprice"]

       id: int
       name: str
       duration: int
       bytes: float
       unitprice: int


   if __name__ == "__main__":
       if len(sys.argv) > 1:
           pgconn = psycopg.connect(PGCONNSTRING)
           artist = Artist.fetchone(pgconn, name=sys.argv[1])
           for album in Album.fetchall(pgconn, artistid=artist.id):
               ms = 0
               for track in Track.fetchall(pgconn, albumid=album.id):
                   ms += track.duration
                   duration = timedelta(milliseconds=ms)
               print("%25s: %s" % (album.title, duration))
       else:
           print("albums.py <artist name>")
   #+end_src

   Now the result of this code is as following:
   #+begin_src sh
   $ uv run main.py "Red Hot Chili Peppers"
   Blood Sugar Sex Magik: 1:13:57.073000
   By The Way: 1:08:49.951000
   Californication: 0:56:25.461000
   #+end_src
** Correctness
   When using multiple statements, it is necessary to setup the /isolation level/ correctly.
   Also, the connection and transaction semantice of your code should be tightly controlled.

   The SQL standard default four isolation level and PostgreSQL implements three of them,
   leaving out /dirty reads/.

   Think of the isolation levels like this:
   - Read uncommited \\
     PostgreSQL accepts this setting and actually implements /read commited/ here,
     which is compliant with the SQL standard
   - Read committed \\
     This is the default and it allows your transaction to see other transactions
     changes as soon as they are committed; it means that if you run the following
     query twice in your transaction but someone else added or removed objects
     from the stock, you will have different count at different points in your transaction
     #+begin_src sql
     select count(*) from stock;
     #+end_src
   - Repeatable read \\
     In this isolation level, your transaction keeps the same /snapshot/ of the whole database
     for its entire duration, from *BEGIN* to *COMMIT*.
     It is very useful to have that for online backups - a straightforward use case for this
     feature.
   - Serializable \\
     This level guarantees that a one-transaction-at-a-time ordering of what happens on the server
     exists with the exact same result as what you're obtaining with concurrent activity.

   By default working in /read commited/ isolation level.

   Each running transaction in a PostgreSQL system can hae a different isolation level.

** Efficiency
   The correct soultion is eight lines of very basic SQL.

   In the application's code solution, here's what happens under the hood:
   - First, we fetch the artist from the database, so that's one network round trip
     and one SQL query that returns the artist id and its name

     note that we don't need the name of the artist in our use-save, so that's a useless
     amount of bytes sent on the network, and also in memory in the application
   - Then we do another network round-trip to fetch a list of albums for the artistid
     we just retrieved in the previous query, and store the result in the application's
     memory
   - Now for each album we send another SQL query via the network to the database server
     and fetch the list of tracks and their properties, including the duration in milliseconds.
   - In the same loop where we fetch the tracks durations in milliseconds, we sum them up
     in the application's memory - we can approximate the CPU usage on the application side
     to be the same as the one in the PostgreSQL server.
   - Finnaly, the application can output the fetched data
** Stored Procedures - a Data Access API
   When using PostgreSQL it is also possible to create server-side functions.
   #+begin_src sql
   create or replace function get_all_albums
   (
   in artistid bigint,
   out album text,
   out duration interval
   )
   returns setof record
   language sql
   as $$
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.artistid = get_all_albums.artistid
   group by album
   order by album;
   $$;
   #+end_src

   Then we can use this procedure with /lateral/ join technique:
   #+begin_src sql
   select album, duration
   from artist
   lateral get_all_albums(artistid)
   where artist.name = 'Red Hot Chili Peppers';
   #+end_src

   And example we want to list the album with durations of the artists
   who have exactly four albums registered in database:
   #+begin_src sql
   with four_albums as
      (
      select artistid
      from album
      group by artistid
      having count(*) = 4
   )
   select artist.name, album, duration
   from four_albums
   join artist using(artistid),
   lateral get_all_albums(artistid)
   order by artistid, duration desc;
   #+end_src
** Procedural Code and Stored Procedures
   If you want to use stored procedures, please always write them in SQL,
   and only switch to /PLpgSQL/ when necessary.
   If you want to be efficient, the default should be SQL.

* A Small Application

** Music Catalog
   Using the [[https://github.com/nackjicholson/aiosql][aiosql]] Python library it is very
   easy to embed SQL code in Python and keep the SQL clean and tidy in .sql files.

   The artist.sql file looks like this:
   #+INCLUDE: "./assets/python/taop_book/chapter5/music_catalog/queries/artist.sql" src python
