:PROPERTIES:
:ID: 0ec0df6d-941f-40ff-9dee-bc56c521e53b
:BOOK_URL: [[https://storage.sbg.cloud.ovh.net/v1/AUTH_e5524010dbdf45ccb5cdac68b254c4f7/TAOP/TAOP-Volume-1.pdf]]
:END:
#+TITLE: The Art of PostgreSQL book

* 1. Structured Query Language (SQL)
  RDBMS (Relational DataBase Management System) and SQL are forcing developers to think in terms of data structure,
  and to declare both the data structure and the data set we want to obain
  via our queries.

  #+begin_quote
  Bad programmers worry about the code. \\  
  Good programmers worry about data structures and their relationships.

  --- Linus Torvalds
  #+end_quote
** Some of the Code is Written in SQL
   The current SQL standard is SQL:2016.

   If your application is already using the SQL programming language
   and SQL engine, then as a developer it's important to fully understand how 
   much can be achieved in SQL, and what service is implemented by this runtime
   dependency in your software architecture.

   SQL is a very powerful programming language, and it is a declarative one.
   It's a wonderful tool to master, and once used properly it allows one to reduce
   both code size and the development time for new features.

** A First Use Case
   Fetch NYSE /Excel/ file and load it into a PostgreSQL table.

   How file looks:
   #+begin_example
   2010  1/4/2010    1,425,504,460  4,628,115   $38.495.460,645
   2010  1/5/2010    1,754,011,760  5,394,016   $43.932.043,406
   2010  1/6/2010    1,655,507,953  5,494,460   $43.816.749,660
   #+end_example

   #+begin_src sql 
   begin;

   drop table if exists factbook;

   create table factbook
   (
      year int,
      date date,
      shares text,
      trades text,
      dollars text
   );

   \copy factbook from 'factbook.csv' with delimiter E'\t' null ''

   alter table factbook
   alter shares
   type bigint
   using replace(shares, ',', '')::bigint,

   alter trades
   type bigint
   using replace(trades, ',', '')::bigint,

   alter dollars
   type bigint
   using substring(replace(dollars, ',', '') from 2)::numeric;

   commit;
   #+end_src


** Application Code and SQL
   Query that list all entries we have in the month of February 2017:
   #+begin_src sql
   \set start '2017-02-01'

   select date, to_char(shares, '99G999G999G999') as shares, to_char(trades, '99G999G999') as trades, to_char(dollars, 'L99G999G999G999') as dollars
   from factbook
   where date >= date :'start'
   and date < date :'start' + interval '1 month'
   order by date;
   #+end_src

   Result of the query:
   | date       | shares        | trades    | dollars          |
   |------------+---------------+-----------+------------------|
   | 2017-02-01 | 1,161,001,502 | 5,217,859 | $ 44,660,060,305 |
   | (n rows)   |               |           |                  |

   Typical implementation of that expectation in Python
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg-binary",
   # ]
   # ///
   from datetime import date
   import sys
   import psycopg2
   import psycopg2.extras
   from calendar import Calendar

   CONNSTRING = "dbname=yesql application_name=factbook"


   def fetch_month_data(year: int, month: int) -> dict[date, tuple[str, str, str]]:
       date = "%d-%02d-01" % (year, month)
       sql = """
   select date, shared, trades, dollars
   from factbook
   where date >= date %s
   and date < date %s + interval '1 month'
   order by date;
   """
       pgconn = psycopg2.connect(CONNSTRING)
       curs = pgconn.cursor()
       curs.execute(sql, (date, date))

       return {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in curs.fetchall()
       }


   def list_book_for_month(year: int, month: int):
       data = fetch_month_data(year, month)
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades", "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" * 12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src

   Output when running the program
   #+begin_src sh
   $ uv run main.py 2017 2
   | day        | shares     | trades  | dollars     |
      |------------+------------+---------+-------------|
      | 2017-02-01 | 1161001502 | 5217859 | 44660060305 |
      | etc
   #+end_src
** A Word about SQL Injection
   [[https://imgs.xkcd.com/comics/exploits_of_a_mom.png]]

   It is advisable that read the documentation of current driver and understand how to send
   SQL query parameters separately from the main SQL query text;
   this is a reliable way to never have to worry about SQL injection problems ever again.

   In particular, never build a query string by concatenating query arguments directly
   into query strings, i.e. in the application client code.
   Never use library, ORM or another tooling that would do that.

   We were using the psycopg Python driver which is based on *libpq*. \\  
   A lot of PostgreSQL application drivers are based on the libpq C driver, which
   implements the PostgreSQL protocols and is mantained alongside the main server's code.


** PostgreSQL protocol: server-side prepared statements
   Server-side Prepared Statements can be used in SQL thanks to the *PREPARE*
   and *EXECUTE* commands syntax:
   #+begin_src sql
   prepare foo as
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;

   -- And then execute the prepared statement with a parameter
   execute foo('2010-02-01') 
   #+end_src

   Remember: SQL injection happens when the SQL parser is fooled into beliving that
   a parameter string is in fact a SQL query, and then the SQL engine goes on and
   execute that SQL statement.

   *asyncpg* PostgreSQL driver that implements the PostgreSQL protocol itself, and uses
   server-side prepared statements.

   This example is now safe from SQL injection by design, because the server-side
   prepared statement protocol sends the query string and its arguments in separate protocol
   messages:
   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "asyncpg",
   # ]
   # ///
   import sys
   import asyncio
   import asyncpg
   import datetime
   from calendar import Calendar

   CONNSTRING = "postgresql://appdev@localhost/appdev?application_name=factbook"


   async def fetch_month_data(year: int, month: int):
       date = datetime.date(year, month, 1)
       sql = """
   select date, shares, trades, dollars
   from factbook
   where date >= $1::date
   and date < $1::date + interval '1 month'
   order by date;
   """
       pgconn = await asyncpg.connect(CONNSTRING)
       stmt = await pgconn.prepare(sql)
       res = {
           date: (shares, trades, dollars)
           for (date, shares, trades, dollars) in stmt.fetch(date)
       }
       await pgconn.close()
       return res


   def list_book_for_month(year: int, month: int):
       data = asyncio.run(fetch_month_data(year, month))
       cal = Calendar()
       print("%12s | %12s | %12s | %12s" % ("day", "shares", "trades", "dollars"))
       print("%12s-+-%12s-+-%12s-+-%12s" % ("-" * 12, "-" * 12, "-" * 12, "-" * 12))

       for day in cal.itermonthdates(year, month):
           if day.month != month:
               continue
           if day in data:
               shares, trades, dollars = data[day]
           else:
               shares, trades, dollars = 0, 0, 0
           print("%12s | %12s | %12s | %12s" % (day, shares, trades, dollars))


   if __name__ == "__main__":
       year = int(sys.argv[1])
       month = int(sys.argv[2])
       list_book_for_month(year, month)
   #+end_src


** Back to Discovering SQL
   Now of course it's possible to implement the same expectations with a single SQL
   query, without any application code being spent on solving the problem:
   #+begin_src sql
   select cast(calendar.entry as date) as date,
      coalesce(shares, 0) as shares,
      coalesce(trades, 0) as trades,
      to_char(
      coalesce(dollars,0),
      'L99G999G999G999'
      ) as dollars
   from
   generate_series(date :'start',
   date :'start' + interval '1 month'
   - interval '1 day',
   interval '1 day'
   )
   as calendar(entry)
   left join factbook
   on factbook.date = calendar.entry
   order by date;
   #+end_src

   Here's the result of running this query:
   | date       | shares     | trades  | dollars          |
   |------------+------------+---------+------------------|
   | 2017-02-01 | 1161001502 | 5217859 | $ 44,660,060,305 |
   | 2017-02-02 | 1128144760 | 4586343 | $ 43,276,102,903 |
   | etc        |            |         |                  |

   Note that we replaced 60 lines of Python code with a simple enough SQL query
   Here, the Python is doing and /Hash Join Nested Loop/ where PostgreSQL picks a 
   /Merge Left Join/ over two ordered relations.

* 2. Software Architecture
  When designing your software architecture, you need to think about PostgreSQL
  not as /storage/ layer, but rather as a /concurrent data access service/.
  This service is capable of handling data processing.

** Why PostgreSQL? 
   That choice is down to several factors, all consequences of PostgreSQL
   truly being /the world's most advanced open source database/:
   - PostgreSQL is open source, available under a BSD like licence
     named the *PostgreSQL licence*
   - The PostgreSQL project is done completely in the open. \\  
     The project goes as far as self-hosting all requirements in order
     to avoid being influenced by a particular company.
   - PostgreSQL releases a new major version about once a year, following a
     /when it's ready/ releaase cycle
   - The PostgreSQL design allows enhancing SQL in very advanced ways


* 3. Getting Ready to read this Book
  #+begin_src sh
  $ docker run -d -p 127.0.0.1:5432:5432 -e POSTGRES_PASSWORD="1234" --name the_art_of_postgresql_book postgres:16.9-alpine
  $ docker exec -it the_art_of_postgresql_book psql -U postgres
  postgres=# show server_version;
  server_version 
  ----------------
  16.9
  (1 row)
  #+end_src

  Need to import /Chinook/ database to PostgreSQL:
  #+begin_src sh
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -c "create database appdev"
  $ curl -fsSL "https://github.com/edpyt/TAOP-sql/raw/3a7c45887a4223a730a7659cb2325990d0696cfd/TheArtOfPostgreSQL-database-sql/chinook.sql" | docker exec -i the_art_of_postgresql_book psql -U postgres -d appdev 
  $ docker exec -it the_art_of_postgresql_book psql -U postgres -d appdev -c "ALTER ROLE postgres SET search_path TO chinook;"
  #+end_src
* 4. Business Logic

** Every SQL query embeds some business logic
   Each and every and all SQL query contains some levels of business logic.

   Example:
   #+begin_src sql
   select name
   from track
   where albumid = 193
   order by trackid;
   #+end_src
   What business logic is embedded in that SQL statement?
   - The /select/ clause only mentions the /name/ column, and that's relevant
     to your application.
     In the situation in which your application runs this query, the business logic
     is only interested into the tracks names.
   - The /from/ clause only mentions the /track/ trable, somehow we decided that's
     all we need in this example, and that again is strongly tied to the logic being
     implemented
   - The /where/ clause restricts the data output to the /albumid/ 193 which again
     is a direct translation of our business logic, with the added information
     that the album we want now is the 193rd one and we're left to wonder how 
     we know about it
   - Finally, the /order by/ clause implements the idead that we want to display
     the track names in the order they appear on the disk.
     Not only that, it also incorporates the specific knowledge that the /trackid/
     column ordering is the same as the original disk ordering of the tracks.
** Business Logic Applies to Use Cases
   Display the list of albums from a given artist, each with its total duration.
   #+begin_src sql
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.name = 'Red Hot Chili Peppers'
   group by album
   order by album;
   #+end_src
   The output is:
   | album                 | duration     |
   |-----------------------+--------------|
   | Blood Sugar Sex Magik | 01:13:57.073 |
   | By The Way            | 01:08:49.951 |
   | Californication       | 00:56:25.461 |

   What we see here is a direct translation from the business case (or user story)
   into a SQL query. The SQL implementation uses joins and computations that are specific
   to both the data model and the use case we are solving.

   Another implementation could be done with several queries and the computation
   in the application's main code:
   1. Fetch the list of albums for the selected artist
   2. For each album, fetch the duration of every track in the album
   3. In the application, sum up the durations per album

   #+begin_src python
   # /// script
   # requires-python = ">=3.13"
   # dependencies = [
   #     "psycopg[binary]",
   # ]
   # ///
   from dataclasses import dataclass
   from typing import ClassVar, Self
   import psycopg
   import sys
   from datetime import timedelta

   from psycopg.rows import dict_row

   DEBUGSQL = False
   PGCONNSTRING = "host=localhost user=postgres password=1234 dbname=appdev"


   @dataclass
   class Model:
       tablename: ClassVar[str | None] = None
       columns: ClassVar[list[str]] = []

       @classmethod
       def buildsql(cls, pgconn: psycopg.Connection, **kwargs) -> str | None:
           if cls.tablename and kwargs:
               cols = ", ".join('"%s"' % c for c in cls.columns)
               qtab = '"%s"' % cls.tablename
               sql = "select %s from %s where" % (cols, qtab)
               for key in kwargs.keys():
                   sql += "\"%s\" = '%s'" % (key, kwargs[key])
                   if DEBUGSQL:
                       print(sql)
                   return sql

       @classmethod
       def fetchone(cls, pgconn: psycopg.Connection, **kwargs) -> Self | None:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if (result := curs.fetchone()) is not None:
                   return cls(*result.values())

       @classmethod
       def fetchall(cls, pgconn: psycopg.Connection, **kwargs) -> list[Self]:
           if (
               (cls.tablename and kwargs)  #
               and (sql := cls.buildsql(pgconn, **kwargs))
           ):
               curs = pgconn.cursor(row_factory=dict_row)
               curs.execute(sql)  # type: ignore[reportArgumentType]
               if resultset := curs.fetchall():
                   return [cls(*result.values()) for result in resultset]
           return []


   @dataclass
   class Artist(Model):
       tablename = "artist"
       columns = ["artistid", "name"]

       id: int
       title: str


   @dataclass
   class Album(Model):
       tablename = "album"
       columns = ["albumid", "title"]

       id: int
       title: str
       duration: float | None = None


   @dataclass
   class Track(Model):
       tablename = "track"
       columns = ["trackid", "name", "milliseconds", "bytes", "unitprice"]

       id: int
       name: str
       duration: int
       bytes: float
       unitprice: int


   if __name__ == "__main__":
       if len(sys.argv) > 1:
           pgconn = psycopg.connect(PGCONNSTRING)
           artist = Artist.fetchone(pgconn, name=sys.argv[1])
           for album in Album.fetchall(pgconn, artistid=artist.id):
               ms = 0
               for track in Track.fetchall(pgconn, albumid=album.id):
                   ms += track.duration
                   duration = timedelta(milliseconds=ms)
               print("%25s: %s" % (album.title, duration))
       else:
           print("albums.py <artist name>")
   #+end_src

   Now the result of this code is as following:
   #+begin_src sh
   $ uv run main.py "Red Hot Chili Peppers"
   Blood Sugar Sex Magik: 1:13:57.073000
   By The Way: 1:08:49.951000
   Californication: 0:56:25.461000
   #+end_src
** Correctness
   When using multiple statements, it is necessary to setup the /isolation level/ correctly.
   Also, the connection and transaction semantice of your code should be tightly controlled.

   The SQL standard default four isolation level and PostgreSQL implements three of them,
   leaving out /dirty reads/.

   Think of the isolation levels like this:
   - Read uncommited \\  
     PostgreSQL accepts this setting and actually implements /read commited/ here,
     which is compliant with the SQL standard
   - Read committed \\  
     This is the default and it allows your transaction to see other transactions
     changes as soon as they are committed; it means that if you run the following
     query twice in your transaction but someone else added or removed objects
     from the stock, you will have different count at different points in your transaction
     #+begin_src sql
     select count(*) from stock;
     #+end_src
   - Repeatable read \\  
     In this isolation level, your transaction keeps the same /snapshot/ of the whole database
     for its entire duration, from *BEGIN* to *COMMIT*.
     It is very useful to have that for online backups - a straightforward use case for this
     feature.
   - Serializable \\  
     This level guarantees that a one-transaction-at-a-time ordering of what happens on the server
     exists with the exact same result as what you're obtaining with concurrent activity.

   By default working in /read commited/ isolation level.

   Each running transaction in a PostgreSQL system can hae a different isolation level.

** Efficiency
   The correct soultion is eight lines of very basic SQL.

   In the application's code solution, here's what happens under the hood:
   - First, we fetch the artist from the database, so that's one network round trip
     and one SQL query that returns the artist id and its name

     note that we don't need the name of the artist in our use-save, so that's a useless
     amount of bytes sent on the network, and also in memory in the application
   - Then we do another network round-trip to fetch a list of albums for the artistid
     we just retrieved in the previous query, and store the result in the application's
     memory
   - Now for each album we send another SQL query via the network to the database server
     and fetch the list of tracks and their properties, including the duration in milliseconds.
   - In the same loop where we fetch the tracks durations in milliseconds, we sum them up
     in the application's memory - we can approximate the CPU usage on the application side
     to be the same as the one in the PostgreSQL server.
   - Finnaly, the application can output the fetched data
** Stored Procedures - a Data Access API
   When using PostgreSQL it is also possible to create server-side functions.
   #+begin_src sql
   create or replace function get_all_albums
   (
   in artistid bigint,
   out album text,
   out duration interval
   )
   returns setof record
   language sql
   as $$
   select album.title as album,
      sum(milliseconds) * interval '1 ms' as duration
   from album
   join artist using(artistid)
   left join track using(albumid)
   where artist.artistid = get_all_albums.artistid
   group by album
   order by album;
   $$;
   #+end_src

   Then we can use this procedure with /lateral/ join technique:
   #+begin_src sql
   select album, duration
   from artist
   lateral get_all_albums(artistid)
   where artist.name = 'Red Hot Chili Peppers';
   #+end_src

   And example we want to list the album with durations of the artists
   who have exactly four albums registered in database:
   #+begin_src sql
   with four_albums as
      (
      select artistid
      from album
      group by artistid
      having count(*) = 4
   )
   select artist.name, album, duration
   from four_albums
   join artist using(artistid),
   lateral get_all_albums(artistid)
   order by artistid, duration desc;
   #+end_src
** Procedural Code and Stored Procedures
   If you want to use stored procedures, please always write them in SQL,
   and only switch to /PLpgSQL/ when necessary.
   If you want to be efficient, the default should be SQL.

* 5. A Small Application

** Music Catalog
   Using the [[https://github.com/nackjicholson/aiosql][aiosql]] Python library it is very
   easy to embed SQL code in Python and keep the SQL clean and tidy in .sql files.

   Needed .sql files can be found here: [[./assets/python/taop_book/chapter5/music_catalog/queries/][queries]]

* 6. The SQL REPL - An Interactive Setup
  New uses of PostgreSQL often want to find an andvanced visual query editing
  tool and are confused when /psql/ is the answer.
  Most PostgreSQL advanced users and experts don't even think about it and use
  /psql/.

  /psql/ implements a REPL (read-eval-print loop)

** The psqlrc Setup
   Full setup of /psql/ that being used:
   #+begin_src psqlrc
   -- Recommended psql config from Chapter 6: The SQL REPL — An Interactive Setup

   -- These set commands are noisy; let's shush 'em.
   -- \set QUIET ON

   \set PROMPT1 '%~%x%# '
   \x auto
   \set ON_ERROR_STOP on
   \set ON_ERROR_ROLLBACK interactive

   \pset null '¤'
   \pset linestyle 'unicode'
   \pset unicode_border_linestyle single
   \pset unicode_column_linestyle single
   \pset unicode_header_linestyle double
   \set intervalstyle to 'postgres_verbose';

   \setenv LESS '-iMFXSx4R'
   -- Take your pick: emacs, nano, vim, or install another
   \setenv EDITOR 'vim'

   \set QUIET OFF
   #+end_src
*** My setup with Docker
    #+begin_src bash
    $ docker exec -it the_art_of_postgresql_book apk add vim
    $ docker exec -it the_art_of_postgresql_book bash -c "wget -P ~/ \
       https://raw.githubusercontent.com/mikebranski/the-art-of-postgresql-docker/refs/heads/master/.psqlrc"
    #+end_src

** Transactions and psql Behavior 
   We set several /psql/ variables that change its behavior:
   - \set ON_ERROR_STOP on \\  
     It allows /psql/ to know that it is not to continue trying to execute all your
     commands when a previous one is throwing an error.
     It's primarily practical for scripts and be also set using the command line.
   - \set ON_ERROR_ROLLBACK interactive \\  
     This settings changes how /psql/ behaves with respect to transactions.
     It is a very good interactive setup, and must be avoided in batch scripts.

     From the documentation, when set to:
     - *on* \\  
       If a statement in a transaction block generates an error, the error is
       ignored and the transaction continues.
     - *interactive* \\  
       Such errors are only ignored in interactive sessions, and not when reading 
       script files.
     - *unset* or set to *off* \\  
       a statement in a transaction block that generates an error aborts the entire
       transaction.

   The error rollback mode works by issuing an implicit *SAVEPOINT*, just before each
   command that is in a transaction block, and then rolling back to the savepoint
   if the command fails.

   With the /\set PROMPT1 '%~%*%# '/, /psql/ displays a little star in the prompt
   when there's a transaction in flight.

   Example output with /ON_ERROR_ROLLBACK/ set to:
   - *off*
     #+begin_src
     appdev# \set ON_ERROR_ROLLBACK off
     appdev# begin;
     BEGIN
     appdev*# select 1/0;
     ERROR:  division by zero
     appdev!# select 1+1;
     ERROR:  current transaction is aborted, commands ignored until end of transaction block
     appdev!# rollback;
     ROLLBACK  
     #+end_src
   - *interactive*
     #+begin_src
     appdev# \set ON_ERROR_ROLLBACK interactive
     appdev# begin;
     BEGIN
     appdev*# select 1/0;
     ERROR:  division by zero
     appdev*# select 1+1;
     ?column? 
     ══════════
           2
     (1 row)

     appdev*# commit;
     COMMIT
     #+end_src

   For discover database schema use /\l+/:
   #+begin_src
   appdev# \set ECHO_HIDDEN true
   appdev# \l+
   ********* QUERY **********
   SELECT
   d.datname as "Name",
   pg_catalog.pg_get_userbyid(d.datdba) as "Owner",
   pg_catalog.pg_encoding_to_char(d.encoding) as "Encoding",
   CASE d.datlocprovider WHEN 'c' THEN 'libc' WHEN 'i' THEN 'icu' END AS "Locale Provider",
   d.datcollate as "Collate",
   d.datctype as "Ctype",
   d.daticulocale as "ICU Locale",
   d.daticurules as "ICU Rules",
   pg_catalog.array_to_string(d.datacl, E'\n') AS "Access privileges",
   CASE WHEN pg_catalog.has_database_privilege(d.datname, 'CONNECT')
         THEN pg_catalog.pg_size_pretty(pg_catalog.pg_database_size(d.datname))
         ELSE 'No Access'
   END as "Size",
   t.spcname as "Tablespace",
   pg_catalog.shobj_description(d.oid, 'pg_database') as "Description"
   FROM pg_catalog.pg_database d
   JOIN pg_catalog.pg_tablespace t on d.dattablespace = t.oid
   ORDER BY 1;
   **************************

   List of databases
   ─[ RECORD 1 ]─────┬───────────────────────────────────────────
   Name              │ appdev
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   Collate           │ en_US.utf8
   Ctype             │ en_US.utf8
   ICU Locale        │ 
   ICU Rules         │ 
   Access privileges │ 
   Size              │ 24 MB
   Tablespace        │ pg_default
   Description       │ 
   ═[ RECORD 2 ]═════╪═══════════════════════════════════════════
   Name              │ postgres
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   Collate           │ en_US.utf8
   Ctype             │ en_US.utf8
   ICU Locale        │ 
   ICU Rules         │ 
   Access privileges │ 
   Size              │ 7361 kB
   Tablespace        │ pg_default
   Description       │ default administrative connection database
   ═[ RECORD 3 ]═════╪═══════════════════════════════════════════
   Name              │ template0
   Owner             │ postgres
   Encoding          │ UTF8
   Locale Provider   │ libc
   appdev# \l+
   appdev# \set ECHO_HIDDEN false
   appdev# select datname,
   appdev-# pg_database_size(datname) as bytes
   appdev-# from pg_database
   appdev-# order by bytes desc;
   datname  │  bytes   
   ═══════════╪══════════
   appdev    │ 25596387
   template1 │  7602703
   postgres  │  7537167
   template0 │  7537167
   (4 rows)
   #+end_src

   For edit last SQL query in visual editor:
   #+begin_src
   appdev# select datname,
   pg_database_size(datname) as bytes
   from pg_database
   order by bytes desc;
   datname  │  bytes   
   ═══════════╪══════════
   appdev    │ 25596387
   template1 │  7602703
   postgres  │  7537167
   template0 │  7537167
   (4 rows)

   appdev# \e
   select datname,
   pg_database_size(datname) as bytes
   from pg_database
   order by bytes desc;
   "/tmp/psql.edit.334.sql" 4L, 89B                                                                           1,1           All
   #+end_src
* 7. SQL is Code
  We approached a good way to have SQL queries as .sql files in code base.

  Now that SQL is actually code in application's source tree, we need to apply
  the ssame methodoloy that you're used to: set a minimum lefel of expected quality,
  code comments, consistent naming, unit testing and code revision systems.
** SQL style guidelines 
   A few examples of bad and good style:
   - 1.
     #+begin_src sql
     SELECT title, name FROM albume LEFT JOIN track USING(albumid) WHERE albumid = 1 ORDER BY 2
     #+end_src
     Query is using the old habit of all-caps SQL keywords. \\  
     We now have color screens and syntax highlighting and we don't write all-caps
     code anymore
   - 
     #+begin_src sql
     select title, name
     from album left join track using(albumid)
     where albumid=1
     order by 2;
     #+end_src
     Now it's quite a bit easier to understand the structure of this query at a glance and to realize
     that it is indeed a very basic SQL statement.


* 8. Indexing Strategy
** Indexing for Constraints 
   PostgreSQL provides a rich set of tools for developers to manage concurrent access to data.
   Internally, data consistency is mantained by using a multiversion model (Multiversion Concurrency Control,
   MVCC).
   This meands that each SQL statement sees a snapshot of data (a database version) as it was some time
   ago, regardless of the current state of the underlying data.

   Example with two transactions /t1/ and /t2/ happening in parallel:
   #+begin_src
   t1> insert into test(id) values(1);
   t2> insert into test(id) values(1);
   #+end_src
   One of them has to be refused, because they are conflicting with the one another.
   PostgreSQL knows how to do that, and thee implementation relies on the internal code being
   able to acceess the indexes in a non-MVCC compliant way: the internal code of PostgreSQL knows
   what the in-flight non-committed thansactions are doing.

   The way the internals of PostgreSQL solve this problem is by relying on its index data structure in a
   non-MVCC compliant way, and this capability is not visible to SQL level users.

   So when you declare a /unique/ constraint, a /primary key/ constrain or an /exclusion constraint/
   PostgreSQL creates an index for you:
   #+begin_src
   appdev# create table test(id integer unique);
   CREATE TABLE
   appdev# \d test
                  Table "chinook.test"
   Column │  Type   │ Collation │ Nullable │ Default 
   ════════╪═════════╪═══════════╪══════════╪═════════
   id     │ integer │           │          │ 
   Indexes:
      "test_id_key" UNIQUE CONSTRAINT, btree (id)
   #+end_src

** Indexing for Queries

   PostgreSQL automatically creates only those indexes that are needed for the system
   to behave correctly.
   Any and all other indexes are to be defined by the *application developers*
   when they need a faster access method to some tuples.

   An index cannot alter the result of query. An index only provides another access
   method to the data, one that is faster than a sequential scan in most cases.
   Query semantics and result set don't depend on indexes.

   Implementing a user story (or a business case) with the help of SQL queries is
   the job of the developer.
   As the authore of the SQL statements, the developer also should be responsible for
   choosing which indexes are needed to support their queries.
** PostgreSQL Index Access Methods
   An /access method/ is a generic algorithm with a clean API that can be imlemented
   for compatible data types.

   #+begin_quote
   PostgreSQL provides several index types: B-tree, Hash, GiST, SP-GiST, GIN and BRIN.
   Each index type uses a different algorithm that is best suited to different types of queries.
   By default, the *CREATE INDEX* command creates B-tree indexes, which fit the most common
   situations.
   #+end_quote

   Each index access method has been designed to solve specific use case:
   - /B-Tree/, or balanced tree \\  
     Balanced indexes are the most common used, by a long shot, because they are very
     efficient and provide an algorithm that applies to most cases.
   - /GiST/, or generalized search tree \\  
     Its implementation in PostgreSQL allows support for 2-dimensional data types such
     as the geometry /point/ or the /ranges/ data types. 
   - /SP-GiST/, or spaced partitioned gist \\  
     /SP-GiST/ indexes are the only PostgreSQL index access method implementation that
     support non-balanced disk-based data structures, such as quadtrees, k-d trees, and
     radix trees (tries).
     This is useful when you want to index 2-dimensional data with very different densities.
   - /GIN/, or generalized inverted index \\  
     /GIN/ is designed for handling cases where the items to be indexed are composite values,
     and the queries to be handled by the index need to search for element values that appear
     within the composite items. \\  
     /GIN/ indexes are "inverted indexes" which are appropriate for data values that contain
     multiple component values, such as arrays. An inverted index contains a separate entry
     for each component value.
   - /BRIN/, or block range indexes \\  
     Store summaries about the values stored in consecutive physical block ranges of a table.
     Can support many different indexing strategies, and the particular operators with which a
     BRIN index can be used vary depending on the indexing strategy.
     For data types that have a linear sort order, the indexed data corresponds to the minimum
     and maximum values of the values in the column for each block range.
   - /Hash/ \\  
     Can only handle simple equality comparisons. 
   - /Bloom filters/ \\  
     Is a space-efficient data structure that is used to test whether an element is a member
     of a set. \\  
     This type of index is most useful when a table has many attributes and queries test arbitrary
     combinations of them.
     A traditional B-tree index is faster that a Bloom index, but it can require many B-tree
     indexes to support all possible queries where one needs only a signle Bloom index. \\  
     Also, they are useful when the queries themselves are referencing most or all of those columns
     in equality comparisons.
** Adding Indexes 
   Not every query needs to be that fast, and the requirements are mostly user defined.

   Indexing needs analysis by listing every query that averages out to more that 10 milliseconds,
   or some other sensible threshold for application.
   The only way to understand where time is spent in a query is by using the *EXPLAIN* command
   and reviewing the /query plan/. \\  
   From the documentation:
   #+begin_quote
   You can use the EXPLAIN command to see what query plan the planner creates for any query.
   Plan-reading is an art that requries some experience to master ...
   #+end_quote

